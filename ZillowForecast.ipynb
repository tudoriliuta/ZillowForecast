{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$('<div id=\"toc\"></div>').css({position: 'fixed', top: '120px', left: 0}).appendTo(document.body);\n",
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js');"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$('<div id=\"toc\"></div>').css({position: 'fixed', top: '120px', left: 0}).appendTo(document.body);\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module '_catboost' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style()\n",
    "\n",
    "import xgboost as xg\n",
    "from xgboost import XGBModel\n",
    "from xgboost import plot_importance\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit, ShuffleSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import cross_validation\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autotime\n",
    "%load_ext line_profiler\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 141 ms\n"
     ]
    }
   ],
   "source": [
    "def plot_data(test, pred, sample, title, width=40, height=10, linewidth=0.5, color1='white', color2='orange'):\n",
    "    \"\"\" Plotting method. \"\"\"\n",
    "    fig = plt.figure(figsize=(width, height))\n",
    "    plt.plot(pred[:sample], color=color1, zorder=4, linewidth=linewidth, label='%s Prediction'%(title))\n",
    "    plt.plot(test[:sample], color=color2, zorder=3, linewidth=linewidth, label='%s True Data'%(title))\n",
    "    plt.title = title\n",
    "    plt.legend()\n",
    "\n",
    "# Frequency count\n",
    "def get_frequency(data):\n",
    "    # Gets the frequency of a column's values in 'data'. Pass on a series.\n",
    "    vals = pd.merge(data.to_frame(), data.value_counts().reset_index(), \n",
    "                    how='left', left_on=data.to_frame().columns[0], right_on='index').iloc[:, -1:].values\n",
    "    return vals\n",
    "  \n",
    "def time_data(data):\n",
    "    data['transactiondate'] = pd.to_datetime(data['transactiondate'])\n",
    "    data['day_of_week']     = data['transactiondate'].dt.dayofweek\n",
    "    data['month_of_year']   = data['transactiondate'].dt.month\n",
    "    data['quarter']         = data['transactiondate'].dt.quarter\n",
    "    data['is_weekend']      = (data['day_of_week'] < 5).astype(int)\n",
    "    data.drop('transactiondate', axis=1, inplace=True)\n",
    "    \n",
    "    print('Added time data')\n",
    "    print('........')\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def column_excluder(data, missing_perc_thresh=0.98):\n",
    "    # Quick clean from https://www.kaggle.com/seesee/concise-catboost-starter-ensemble-plb-0-06435\n",
    "    \n",
    "    exclude_missing = []\n",
    "    exclude_unique = []\n",
    "    num_rows = data.shape[0]\n",
    "    for c in data.columns:\n",
    "        num_missing = data[c].isnull().sum()\n",
    "        if num_missing == 0:\n",
    "            continue\n",
    "        missing_frac = num_missing / float(num_rows)\n",
    "        if missing_frac > missing_perc_thresh:\n",
    "            exclude_missing.append(c)\n",
    "\n",
    "        num_uniques = len(data[c].unique())\n",
    "        if data[c].isnull().sum() != 0:\n",
    "            num_uniques -= 1\n",
    "        if num_uniques == 1:\n",
    "            exclude_unique.append(c)\n",
    "            \n",
    "    to_exclude = list(set(exclude_missing + exclude_unique))\n",
    "    \n",
    "    print('Excluded columns:')\n",
    "    print(to_exclude)\n",
    "    print('........')\n",
    "    \n",
    "    return to_exclude\n",
    "\n",
    "def categorical_features(data):\n",
    "    # Quick categories from https://www.kaggle.com/seesee/concise-catboost-starter-ensemble-plb-0-06435\n",
    "        \n",
    "    cat_feature_inds = []\n",
    "    cat_unique_thresh = 1000\n",
    "    for i, c in enumerate(data.columns):\n",
    "        num_uniques = len(data[c].unique())\n",
    "        if num_uniques < cat_unique_thresh \\\n",
    "            and not 'sqft'   in c \\\n",
    "            and not 'cnt'    in c \\\n",
    "            and not 'nbr'    in c \\\n",
    "            and not 'number' in c:\n",
    "            cat_feature_inds.append(i)\n",
    "\n",
    "    print(\"Categorical features:\")\n",
    "    print([data.columns[ind] for ind in cat_feature_inds])\n",
    "    print('........')\n",
    "    \n",
    "    return cat_feature_inds\n",
    "\n",
    "\n",
    "def complex_features(data):\n",
    "    # Gets counts, label encoding and frequency estimates.\n",
    "    \n",
    "    # Frequency of occurances | length of codes | check if * is present\n",
    "    data['propertyzoningdesc_frq'] = get_frequency(data['propertyzoningdesc'])\n",
    "    data['propertyzoningdesc_len'] = data['propertyzoningdesc'].apply(lambda x: len(x) if pd.notnull(x) else x)\n",
    "    #transactions_shuffled['propertyzoningdesc_str'] = transactions_shuffled['propertyzoningdesc'].apply(lambda x: (1 if '*' in str(x) else 0) if pd.notnull(x) else x)\n",
    "\n",
    "    # Label encoding | length of code\n",
    "    #transactions_shuffled['propertycountylandusecode_enc'] = transactions_shuffled[['propertycountylandusecode']].astype(str).apply(LabelEncoder().fit_transform)\n",
    "    #transactions_shuffled['propertycountylandusecode_len'] = transactions_shuffled['propertycountylandusecode'].apply(lambda x: x if pd.isnull(x) else len(x))\n",
    "\n",
    "    # Zip code area extraction\n",
    "    data['regionidzip_ab']  = data['regionidzip'].apply(lambda x: x if pd.isnull(x) else str(x)[:2]).astype(float)\n",
    "    data['regionidzip_abc'] = data['regionidzip'].apply(lambda x: x if pd.isnull(x) else str(x)[:3]).astype(float)\n",
    "\n",
    "    # Region neighbourhood area extraction\n",
    "    data['regionidneighborhood_ab'] = data['regionidneighborhood'].apply(lambda x: str(x)[:2] if pd.notnull(x) else x).astype(float)\n",
    "\n",
    "    # Rawcensustractandblock transformed\n",
    "    data['code_fips_cnt']  = get_frequency(data['rawcensustractandblock'].apply(lambda x: str(x)[:4]))\n",
    "    data['code_tract_cnt'] = get_frequency(data['rawcensustractandblock'].apply(lambda x: str(x)[4:11]))\n",
    "    data['code_block_cnt'] = get_frequency(data['rawcensustractandblock'].apply(lambda x: str(x)[11:]))\n",
    "    data.drop('rawcensustractandblock', axis=1, inplace=True)\n",
    "    \n",
    "    # Encode string values\n",
    "    data[['propertycountylandusecode', 'propertyzoningdesc']] = data[['propertycountylandusecode', 'propertyzoningdesc']].astype(str).apply(LabelEncoder().fit_transform)\n",
    "    \n",
    "    print('Generating complex features')\n",
    "    print('........')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 543 µs\n"
     ]
    }
   ],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing tax features from 2017\n",
      "Excluded columns:\n",
      "['typeconstructiontypeid', 'pooltypeid10', 'yardbuildingsqft26', 'pooltypeid2', 'basementsqft', 'finishedsquarefeet6', 'finishedsquarefeet13', 'decktypeid', 'poolcnt', 'hashottuborspa', 'buildingclasstypeid', 'fireplaceflag', 'pooltypeid7', 'taxdelinquencyflag', 'poolsizesum', 'storytypeid', 'architecturalstyletypeid']\n",
      "........\n",
      "Added time data\n",
      "........\n",
      "Generating complex features\n",
      "........\n",
      "Categorical features:\n",
      "['airconditioningtypeid', 'buildingqualitytypeid', 'fips', 'heatingorsystemtypeid', 'propertylandusetypeid', 'regionidcity', 'regionidcounty', 'regionidneighborhood', 'regionidzip', 'yearbuilt', 'assessmentyear', 'taxdelinquencyyear', 'day_of_week', 'month_of_year', 'quarter', 'is_weekend', 'propertyzoningdesc_frq', 'propertyzoningdesc_len', 'regionidzip_ab', 'regionidzip_abc', 'regionidneighborhood_ab']\n",
      "........\n",
      "time: 1min 5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "seed = 11\n",
    "np.random.seed(seed)\n",
    "drop_tax = True\n",
    "\n",
    "train2016 = pd.read_csv(\"../Data/train_2016_v2.csv\", parse_dates=[\"transactiondate\"], low_memory=False)\n",
    "train2017 = pd.read_csv('../Data/train_2017.csv', parse_dates=['transactiondate'], low_memory=False)\n",
    "\n",
    "if drop_tax:\n",
    "    # Avoids external bias\n",
    "    print('Removing tax features from 2017')\n",
    "    train2017.iloc[:, train2017.columns.str.startswith('tax')] = np.nan\n",
    "\n",
    "\n",
    "properties2016 = pd.read_csv('../Data/properties_2016.csv', low_memory = False)\n",
    "properties2017 = pd.read_csv('../Data/properties_2017.csv', low_memory = False)\n",
    "\n",
    "sample = pd.read_csv('../Data/sample_submission.csv')\n",
    "\n",
    "transactions2016 = pd.merge(train2016, properties2016, how='left', on=['parcelid']).sample(frac=1)\n",
    "transactions2017 = pd.merge(train2017, properties2017, how='left', on=['parcelid']).sample(frac=1)\n",
    "transactions = pd.concat([transactions2016, transactions2017], axis = 0)\n",
    "\n",
    "#transactions[['propertycountylandusecode', 'propertyzoningdesc']] = transactions[['propertycountylandusecode', 'propertyzoningdesc']].astype(str).apply(LabelEncoder().fit_transform)\n",
    "transactions['taxdelinquencyflag'].replace('Y',1, inplace=True)\n",
    "    \n",
    "# Clean columns\n",
    "to_drop = column_excluder(transactions)\n",
    "transactions.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "# Time data\n",
    "transactions = time_data(transactions)\n",
    "transactions = complex_features(transactions)\n",
    "\n",
    "x_all = transactions.drop(['parcelid', 'propertyzoningdesc', 'propertycountylandusecode', 'fireplacecnt'], axis=1)\n",
    "y_all = transactions['logerror']\n",
    "#x_all.drop(['hashottuborspa' 'taxdelinquencyflag' 'fireplaceflag'], axis=1)\n",
    "#x_all['hashottuborspa'].astype(float, inplace=True)\n",
    "\n",
    "#x_all.fillna(-1, inplace=True)#.astype(str)#.apply(LabelEncoder().fit_transform)\n",
    "\n",
    "x_all.fillna(x_all.median(),inplace = True)\n",
    "\n",
    "ratio = 0.0\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_all, y_all, test_size=ratio)\n",
    "\n",
    "x_train_label = x_train['logerror'].copy()\n",
    "x_train_data = x_train.drop(['logerror'], axis=1).copy()\n",
    "\n",
    "# Drop outliers \n",
    "x_train = x_train[(x_train['logerror'] > -0.4) & (x_train['logerror'] < 0.419)]\n",
    "y_train = x_train['logerror']\n",
    "x_train.drop('logerror', axis=1, inplace=True)\n",
    "x_valid.drop('logerror', axis=1, inplace=True)\n",
    "\n",
    "cat_index = categorical_features(x_train)\n",
    "best_columns = x_train.columns\n",
    "\n",
    "y_mean = np.mean(y_train)\n",
    "\n",
    "del x_all\n",
    "del y_all\n",
    "del transactions\n",
    "del transactions2016\n",
    "del transactions2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression MAE: -0.06962 (+/- 0.00124)\n",
      "time: 1.48 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.2s finished\n"
     ]
    }
   ],
   "source": [
    "# OLS\n",
    "model_lr = LinearRegression()\n",
    "model_lr.fit(x_train_data, x_train_label)\n",
    "#y_pred_lr_valid = model_lr.predict(x_valid)\n",
    "#y_pred_lr_train = model_lr.predict(x_train_data)\n",
    "models['LinearRegression'] = model_lr\n",
    "\n",
    "# Make predictions on both test and validation with OLS and BR\n",
    "#predicted_mae_lr_valid = mean_absolute_error(y_valid, y_pred_lr_valid)\n",
    "#predicted_mae_lr_train = mean_absolute_error(x_train_label, y_pred_lr_train)\n",
    "\n",
    "#print('OLS MAE LR Valid:', predicted_mae_lr_valid, 'Train:', predicted_mae_lr_train)\n",
    "\n",
    "scores = cross_validation.cross_val_score(model_lr, x_train_data, x_train_label, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "print(\"%s MAE: %0.5f (+/- %0.5f)\" % (model_lr.__class__.__name__, scores.mean(), scores.std() * 2))\n",
    "\n",
    "#del y_pred_lr_valid\n",
    "#del y_pred_lr_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0695535899978\n",
      "time: 290 ms\n"
     ]
    }
   ],
   "source": [
    "model_lr.fit(x_train_data, x_train_label)\n",
    "print(mean_absolute_error(x_train_label, model_lr.predict(x_train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0686849059227\n",
      "time: 232 ms\n"
     ]
    }
   ],
   "source": [
    "model_lr.fit(x_train, y_train)\n",
    "print(mean_absolute_error(x_train_label, model_lr.predict(x_train_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BayesianRidge MAE: -0.06950 (+/- 0.00129)\n",
      "time: 2.28 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    2.3s finished\n"
     ]
    }
   ],
   "source": [
    "# BayesianRidge Regression\n",
    "model_br = BayesianRidge(compute_score=True)\n",
    "#model_br.fit(x_train, y_train)\n",
    "#y_pred_br_valid = model_br.predict(x_valid)\n",
    "#y_pred_br_train = model_br.predict(x_train_data)\n",
    "models['BayesianRidge'] = model_br\n",
    "\n",
    "#predicted_mae_br_valid = mean_absolute_error(y_valid,       y_pred_br_valid)\n",
    "#predicted_mae_br_train = mean_absolute_error(x_train_label, y_pred_br_train)\n",
    "\n",
    "#print('BR MAE BayesianRidge Valid: %s \\nTrain: %s' % (predicted_mae_br_valid, predicted_mae_br_train))\n",
    "\n",
    "scores = cross_validation.cross_val_score(model_br, x_train_data, x_train_label, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "print(\"%s MAE: %0.5f (+/- %0.5f)\" % (model_br.__class__.__name__, scores.mean(), scores.std() * 2))\n",
    "\n",
    "\n",
    "#del y_pred_br_valid\n",
    "#del y_pred_br_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0686849059227\n",
      "time: 603 ms\n"
     ]
    }
   ],
   "source": [
    "model_br.fit(x_train_data, x_train_label)\n",
    "print(mean_absolute_error(x_train_label, model_lr.predict(x_train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0686849059227\n",
      "time: 400 ms\n"
     ]
    }
   ],
   "source": [
    "model_br.fit(x_train, y_train)\n",
    "print(mean_absolute_error(x_train_label, model_lr.predict(x_train_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 52.1 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model_rf = RandomForestRegressor(n_jobs=1, random_state=2016, verbose=1, n_estimators=500, max_features=12)\n",
    "#model_rf.fit(x_train, y_train)\n",
    "#y_pred_rf_valid = model_rf.predict(x_valid)\n",
    "#y_pred_rf_train = model_rf.predict(x_train_data)\n",
    "models['RandomForest'] = model_rf\n",
    "\n",
    "#predicted_mae_rf_valid = mean_absolute_error(y_valid,       y_pred_rf_valid)\n",
    "#predicted_mae_rf_train = mean_absolute_error(x_train_label, y_pred_rf_train)\n",
    "\n",
    "#print('BR MAE RandomForest Valid: %s \\nTrain: %s' % (predicted_mae_rf_valid, predicted_mae_rf_train))\n",
    "\n",
    "#scores = cross_validation.cross_val_score(model_rf, x_train, y_train, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "#print(\"%s MAE: %0.5f (+/- %0.5f)\" % (model_rf.__class__.__name__, scores.mean(), scores.std() * 2))\n",
    "\n",
    "#del y_pred_rf_train\n",
    "#del y_pred_rf_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.11 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "model_et = ExtraTreesRegressor(\n",
    "        n_jobs=1, random_state=2016, verbose=1,\n",
    "        n_estimators=500, max_features=12)\n",
    "\n",
    "#model_et.fit(x_train, y_train)\n",
    "#y_pred_et_valid = model_et.predict(x_valid)\n",
    "#y_pred_et_train = model_et.predict(x_train_data)\n",
    "models['ExtraTrees'] = model_et\n",
    "\n",
    "#predicted_mae_et_valid = mean_absolute_error(y_valid,       y_pred_et_valid)\n",
    "#predicted_mae_et_train = mean_absolute_error(x_train_label, y_pred_et_train)\n",
    "\n",
    "#print('BR MAE ExtraTrees Valid: %s \\nTrain: %s' % (predicted_mae_et_valid, predicted_mae_et_train))\n",
    "\n",
    "#scores = cross_validation.cross_val_score(model_et, x_train, y_train, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "#print(\"%s MAE: %0.5f (+/- %0.5f)\" % (model_et.__class__.__name__, scores.mean(), scores.std() * 2))\n",
    "\n",
    "#del y_pred_et_valid\n",
    "#del y_pred_et_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 631 µs\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_svr_rbf  = SVR(kernel='rbf',    C=1e3, gamma=0.1)\n",
    "y_rbf  =  model_svr_rbf.fit(x_train, y_train).predict(x_valid)\n",
    "print('RBF',   mean_absolute_error(y_valid, y_rbf))\n",
    "\n",
    "scores_r = cross_validation.cross_val_score(model_svr_rbf,  x_train_data, x_train_label, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "print(\"%s MAE: %0.5f (+/- %0.5f)\" % (model_svr_rbf.__class__.__name__,  scores_r.mean(), scores_r.std() * 2))\n",
    "\n",
    "models['SVRR'] = model_svr_rbf\n",
    "\n",
    "del y_rbf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.21 ms\n"
     ]
    }
   ],
   "source": [
    "model_svr_lin  = SVR(kernel='linear', C=1e3)\n",
    "#y_lin  =  model_svr_lin.fit(x_train, y_train).predict(x_valid)\n",
    "#print('Linear',mean_absolute_error(y_valid, y_lin))\n",
    "\n",
    "#scores_l = cross_validation.cross_val_score(model_svr_lin,  x_train_data, x_train_label, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "#print(\"%s MAE: %0.5f (+/- %0.5f)\" % (model_svr_lin.__class__.__name__,  scores_l.mean(), scores_l.std() * 2))\n",
    "\n",
    "models['SVRL'] = model_svr_lin\n",
    "\n",
    "#del y_lin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_svr_poly = SVR(kernel='poly',   C=1e3, degree=2)\n",
    "y_poly = model_svr_poly.fit(x_train, y_train).predict(x_valid)\n",
    "print('Poly',  mean_absolute_error(y_valid, y_poly))\n",
    "\n",
    "scores_p = cross_validation.cross_val_score(model_svr_poly, x_train_data, x_train_label, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "print(\"%s MAE: %0.5f (+/- %0.5f)\" % (model_svr_poly.__class__.__name__, scores_p.mean(), scores_p.std() * 2))\n",
    "\n",
    "models['SVRP'] = model_svr_poly\n",
    "\n",
    "del y_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.04 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "model_ab = AdaBoostRegressor()\n",
    "#model_ab.fit(x_train, y_train)\n",
    "#y_pred_ab_valid = model_ab.predict(x_valid)\n",
    "#y_pred_ab_train = model_ab.predict(x_train_data)\n",
    "models['AdaBoost'] = model_ab\n",
    "\n",
    "#predicted_mae_ab_valid = mean_absolute_error(y_valid,       y_pred_ab_valid)\n",
    "#predicted_mae_ab_train = mean_absolute_error(x_train_label, y_pred_ab_train)\n",
    "\n",
    "#print('BR MAE AdaBoost Valid: %s \\nTrain: %s' % (predicted_mae_ab_valid, predicted_mae_ab_train))\n",
    "\n",
    "#scores = cross_validation.cross_val_score(model_ab, x_train, y_train, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "#print(\"%s MAE: %0.5f (+/- %0.5f)\" % (model_ab.__class__.__name__, scores.mean(), scores.std() * 2))\n",
    "\n",
    "#del y_pred_ab_valid\n",
    "#del y_pred_ab_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_booster(x_train, y_train, x_valid, y_valid, cat_index, loss='MAE'):\n",
    "    # Cat booster train and predict\n",
    "    num_ensembles = 5\n",
    "    y_pred_valid = 0.0\n",
    "    y_pred_train = 0.0\n",
    "    \n",
    "    print('Initialising CAT Boost Regression')\n",
    "    for i in tqdm(range(num_ensembles)):\n",
    "        print('Building ensemble', i)\n",
    "        # Use CV, tune hyperparameters\n",
    "        catb = CatBoostRegressor(\n",
    "                iterations=630, learning_rate=0.03,\n",
    "                depth=6, l2_leaf_reg=3,\n",
    "                loss_function=loss,\n",
    "                eval_metric='MAE',\n",
    "                random_seed=i)\n",
    "\n",
    "        catb.fit(x_train, y_train, cat_features=cat_index)\n",
    "\n",
    "        y_pred_valid += catb.predict(x_valid)\n",
    "        y_pred_train += catb.predict(x_train)\n",
    "\n",
    "    y_pred_valid /= num_ensembles\n",
    "    y_pred_train /= num_ensembles\n",
    "\n",
    "    print('Train MAE:', mean_absolute_error(y_train, y_pred_train))\n",
    "    print('Valid MAE:', mean_absolute_error(y_valid, y_pred_valid))\n",
    "    \n",
    "    return catb, y_pred_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_cb, preds = cat_booster(x_train, y_train, x_train_data, x_train_label, cat_index)\n",
    "\n",
    "print('BR MAE CatBoost Valid: %s' % (mean_absolute_error(y_valid, preds)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cb = CatBoostRegressor(\n",
    "            iterations=630, learning_rate=0.03,\n",
    "            depth=6, l2_leaf_reg=3,\n",
    "            loss_function='MAE',\n",
    "            eval_metric='MAE')\n",
    "\n",
    "models['CatBoost'] = model_cb\n",
    "\n",
    "#scores = cross_validation.cross_val_score(model_cb, x_train, y_train, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "#print(\"%s MAE: %0.5f (+/- %0.5f)\" % (model_cb.__class__.__name__, scores.mean(), scores.std() * 2))\n",
    "\n",
    "#del preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.76 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "model_gb = GradientBoostingRegressor(\n",
    "             random_state=2016, verbose=1,\n",
    "             n_estimators=500, max_features=12, max_depth=8,\n",
    "             learning_rate=0.05, subsample=0.8)\n",
    "\n",
    "#model_gb.fit(x_train, y_train)\n",
    "#y_pred_gb_valid = model_gb.predict(x_valid)\n",
    "#y_pred_gb_train = model_gb.predict(x_train_data)\n",
    "models['GradientBoosting'] = model_gb\n",
    "\n",
    "#predicted_mae_gb_valid = mean_absolute_error(y_valid,       y_pred_gb_valid)\n",
    "#predicted_mae_gb_train = mean_absolute_error(x_train_label, y_pred_gb_train)\n",
    "\n",
    "#print('BR MAE GradientBoosting Valid: %s \\nTrain: %s' % (predicted_mae_gb_valid, predicted_mae_gb_train))\n",
    "\n",
    "#scores = cross_validation.cross_val_score(model_gb, x_train, y_train, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "#print(\"%s MAE: %0.5f (+/- %0.5f)\" % (model_gb.__class__.__name__, scores.mean(), scores.std() * 2))\n",
    "\n",
    "#del y_pred_gb_valid\n",
    "#del y_pred_gb_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-d7324b0e08b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#d_valid = xg.DMatrix(x_valid, label=y_valid, missing=-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mxgb_gs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_xgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m#models['XGB'] = xgb_gs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, learning_rates, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m             \u001b[0m_check_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15.3 s\n"
     ]
    }
   ],
   "source": [
    "params_xgb = {\n",
    "    'max_depth':        5,  # shuld be 0.5 to 1% of the examples\n",
    "    'subsample':        1,  # Ratio of observations to be used as samples for each tree\n",
    "    'min_child_weight': 10, # Deals with imbalanced data and prevents overfitting as the value >\n",
    "    'objective':        'reg:linear',\n",
    "    'n_estimators':     1000, # Sequential trees to be modelled.\n",
    "    'eta':              0.1,  # Shrinkage. Typically between 0.1 - 0.2 - learning rate for gradient boost (D:0.3)\n",
    "    'eval_metric':      'mae',\n",
    "    'base_score':       y_mean,\n",
    "}\n",
    "\n",
    "d_train = xg.DMatrix(x_train, label=y_train, missing=-1)\n",
    "#d_valid = xg.DMatrix(x_valid, label=y_valid, missing=-1)\n",
    "\n",
    "xgb_gs = xg.train(params_xgb, d_train, num_boost_round=250, verbose_eval=50)\n",
    "#models['XGB'] = xgb_gs\n",
    "\n",
    "#del d_train\n",
    "#del d_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.02 ms\n"
     ]
    }
   ],
   "source": [
    "def light_gbm_folds(x_train, x_valid, y_train, y_valid, params, num_ensembles):\n",
    "    # Light gbm n ensambles average predictions\n",
    "\n",
    "    y_pred_valid = 0.0\n",
    "    y_pred_train = 0.0\n",
    "    \n",
    "    d_train = lgb.Dataset(x_train, label=y_train)\n",
    "    \n",
    "    print('Initialising Light GBM')\n",
    "    for i in tqdm(range(num_ensembles)):\n",
    "        # Use CV, tune hyperparameters\n",
    "        params['seed'] = i\n",
    "        model_lgb = lgb.train(params, d_train, 430)\n",
    "        \n",
    "        lg_pred_valid = model_lgb.predict(x_valid)\n",
    "        lg_pred_train = model_lgb.predict(x_train)\n",
    "\n",
    "    lg_pred_valid /= num_ensembles\n",
    "    lg_pred_train /= num_ensembles\n",
    "    \n",
    "    print('Train MAE:', mean_absolute_error(y_train, lg_pred_train))\n",
    "    print('Valid MAE:', mean_absolute_error(y_valid, lg_pred_valid))\n",
    "    \n",
    "    return model_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising Light GBM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:01<00:00, 24.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MAE: 0.0530005225883\n",
      "Valid MAE: 0.0688828629839\n",
      "time: 2min 1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import lightgbm as lgb\n",
    "\n",
    "params_lg={\n",
    "    'max_bin'          : 10,\n",
    "    'learning_rate'    : 0.0021, # shrinkage_rate\n",
    "    'boosting_type'    : 'gbdt',\n",
    "    'objective'        : 'regression',\n",
    "    'metric'           : 'mae',      \n",
    "    'sub_feature'      : 0.345 ,   \n",
    "    'bagging_fraction' : 0.85, \n",
    "    'bagging_freq'     : 40,\n",
    "    'num_leaves'       : 512,   # num_leaf\n",
    "    'min_data'         : 500,   # min_data_in_leaf\n",
    "    'min_hessian'      : 0.05,  # min_sum_hessian_in_leaf\n",
    "    'verbose'          : 1\n",
    "}\n",
    "\n",
    "model_lgb = light_gbm_folds(x_train, x_train_data, y_train, x_train_label, params_lg, num_ensembles=5)\n",
    "models['LightGBM'] = model_lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 17.8 s\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.layers import Dropout, BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.noise import GaussianDropout\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "def larger_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(size, input_dim=size, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(size*2, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(size, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mae', optimizer=Adam(lr=4e-3, decay=1e-4))\n",
    "    return model\n",
    "\n",
    "# define wider model\n",
    "def wider_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(size*2, input_dim=size, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mae', optimizer=Adam(lr=4e-3, decay=1e-4))\n",
    "    return model\n",
    "\n",
    "\n",
    "# define base model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(size, input_dim=size, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mae', optimizer=Adam(lr=4e-3, decay=1e-4))\n",
    "    return model\n",
    "\n",
    "def prebuilt_nn():\n",
    "    nn = Sequential()\n",
    "    nn.add(Dense(units = 400 , kernel_initializer = 'normal', input_dim = size))\n",
    "    nn.add(PReLU())\n",
    "    nn.add(Dropout(.4))\n",
    "    nn.add(Dense(units = 160 , kernel_initializer = 'normal'))\n",
    "    nn.add(PReLU())\n",
    "    nn.add(BatchNormalization())\n",
    "    nn.add(Dropout(.6))\n",
    "    nn.add(Dense(units = 64 , kernel_initializer = 'normal'))\n",
    "    nn.add(PReLU())\n",
    "    nn.add(BatchNormalization())\n",
    "    nn.add(Dropout(.5))\n",
    "    nn.add(Dense(units = 26, kernel_initializer = 'normal'))\n",
    "    nn.add(PReLU())\n",
    "    nn.add(BatchNormalization())\n",
    "    nn.add(Dropout(.6))\n",
    "    nn.add(Dense(1, kernel_initializer='normal'))\n",
    "    nn.compile(loss='mae', optimizer=Adam(lr=4e-3, decay=1e-4))\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing neural network data...\n",
      "time: 344 ms\n"
     ]
    }
   ],
   "source": [
    "## Preprocessing\n",
    "print(\"Preprocessing neural network data...\")\n",
    "imputer= Imputer()\n",
    "imputer.fit(x_train.iloc[:, :])\n",
    "x_train_nn = imputer.transform(x_train.iloc[:, :])\n",
    "\n",
    "#imputer.fit(x_valid.iloc[:, :])\n",
    "#x_valid_nn = imputer.transform(x_valid.iloc[:, :])\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train_nn = sc.fit_transform(x_train_nn)\n",
    "#x_valid_nn = sc.transform(x_valid_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "size = x_train_nn.shape[1]\n",
    "# Prebuit KAGGLE Kernel\n",
    "np.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=prebuilt_nn, epochs=5, batch_size=50, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "pipeline.fit(x_train_nn, y_train)\n",
    "models['DNN'] = pipeline\n",
    "\n",
    "#print(mean_absolute_error(y_valid, pipeline.predict(x_valid_nn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    " \n",
    "#x_train = x_train.values\n",
    "#x_valid = x_valid.values\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "x_train_lstm = x_train_data.values.reshape((x_train_data.shape[0], 1, x_train_data.shape[1]))\n",
    "#x_valid_lstm = x_valid.values.reshape((x_valid.shape[0], 1, x_valid.shape[1]))\n",
    " \n",
    "# design network\n",
    "lstm = Sequential()\n",
    "lstm.add(LSTM(50, input_shape=(x_train_lstm.shape[1], x_train_lstm.shape[2])))\n",
    "lstm.add(PReLU())\n",
    "lstm.add(Dropout(.2))\n",
    "lstm.add(Dense(units = 100 , kernel_initializer = 'normal'))\n",
    "lstm.add(PReLU())\n",
    "lstm.add(Dropout(.2))\n",
    "lstm.add(Dense(units = 50 , kernel_initializer = 'normal'))\n",
    "lstm.add(PReLU())\n",
    "lstm.add(Dense(1))\n",
    "lstm.compile(loss='mae', optimizer='adam')\n",
    "# fit network\n",
    "#validation_data=(x_valid_lstm, y_valid)\n",
    "lstm.fit(x_train_lstm, x_train_label, epochs=15, batch_size=50, verbose=1, shuffle=False)\n",
    " \n",
    "# make a prediction\n",
    "#yhat = lstm.predict(x_valid_lstm)\n",
    "models['LSTM'] = lstm\n",
    "mae = mean_absolute_error(y_valid, yhat)\n",
    "print('Test MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/dnc1994/Kaggle-Playground/blob/master/home-depot/ensemble.py\n",
    "import time\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, \\\n",
    "        ExtraTreesRegressor, AdaBoostClassifier\n",
    "from sklearn import grid_search\n",
    "\n",
    "def mean_absolute_error_(ground_truth, predictions):\n",
    "    return mean_absolute_error(ground_truth, predictions)\n",
    "\n",
    "MAE = make_scorer(mean_absolute_error_, greater_is_better=False)\n",
    "\n",
    "params_xgb = {\n",
    "    'max_depth':        5,  # shuld be 0.5 to 1% of the examples\n",
    "    'subsample':        1,  # Ratio of observations to be used as samples for each tree\n",
    "    'min_child_weight': 10, # Deals with imbalanced data and prevents overfitting as the value >\n",
    "    'objective':        'reg:linear',\n",
    "    'n_estimators':     1000, # Sequential trees to be modelled.\n",
    "    'eta':              0.1,  # Shrinkage. Typically between 0.1 - 0.2 - learning rate for gradient boost (D:0.3)\n",
    "    'eval_metric':      'mae'\n",
    "}\n",
    "\n",
    "class Ensemble(object):\n",
    "    \n",
    "    def __init__(self, n_folds, stacker, base_models, include_features, cvgrid):\n",
    "        self.n_folds = n_folds\n",
    "        self.stacker = stacker\n",
    "        self.base_models = base_models\n",
    "        self.features = include_features\n",
    "        self.param_grid = cvgrid\n",
    "    \n",
    "\n",
    "    def fit(self, X, y, include_features=False):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        folds = list(KFold(len(y), n_folds=self.n_folds, shuffle=True, random_state=2016))\n",
    "        S_train = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i, c in enumerate(self.base_models):\n",
    "            print('Fitting For Base Model {} ---'.format(c))       \n",
    "            clf = self.base_models[c]\n",
    "            \n",
    "            for j, (train_idx, test_idx) in enumerate(folds):\n",
    "                print('--- Fitting For Fold %d / %d ---', j + 1, self.n_folds)\n",
    "                X_train = X[train_idx]\n",
    "                y_train = y[train_idx]\n",
    "                X_holdout = X[test_idx]\n",
    "                \n",
    "                if c not in ['XGB', 'LightGBM', 'LSTM']:\n",
    "                    \n",
    "                    clf.fit(X_train, y_train)\n",
    "                    y_pred = clf.predict(X_holdout)[:]\n",
    "                    \n",
    "                    S_train[test_idx, i] = y_pred\n",
    "                    \n",
    "                elif c in ['LSTM']:\n",
    "                    x_train_lstm = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "\n",
    "                    clf.fit(x_train_lstm, y_train, epochs=15, batch_size=50, verbose=1, shuffle=False)\n",
    "                    y_pred = clf.predict(X_holdout.reshape((X_holdout.shape[0], 1, X_holdout.shape[1])))[:]\n",
    "                    \n",
    "                    S_train[test_idx, i] = [i[0] for i in y_pred]\n",
    "\n",
    "                else:\n",
    "                    d_train = xg.DMatrix(X_train, label=y_train, missing=-1)\n",
    "                    d_valid = xg.DMatrix(X_holdout, missing=-1)\n",
    "                    \n",
    "                    clf = xg.train(params_xgb, d_train)\n",
    "                    y_pred = clf.predict(d_valid)[:]\n",
    "                    \n",
    "                    S_train[test_idx, i] = y_pred\n",
    "                    \n",
    "                print('Elapsed: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "            print('Elapsed: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "        print('--- Base Models Trained: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "        grid = grid_search.GridSearchCV(estimator=self.stacker, param_grid=self.param_grid, n_jobs=1, cv=5, verbose=20, scoring=MAE)\n",
    "        \n",
    "        if self.features:\n",
    "            S_train = np.append(X, S_train, 1)\n",
    "            \n",
    "        grid.fit(S_train, y)\n",
    "        \n",
    "        try:\n",
    "            print('Best Params:')\n",
    "            print(grid.best_params_)\n",
    "            print('Best CV Score:')\n",
    "            print(-grid.best_score_)\n",
    "            print('Best estimator:')\n",
    "            print(grid.best_estimator_)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        self.stacker = grid.best_estimator_\n",
    "        \n",
    "        print('--- Stacker Trained: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        folds = list(KFold(len(X), n_folds=self.n_folds, shuffle=True, random_state=2016))\n",
    "        if self.features:\n",
    "            S_test = np.append(X, np.zeros((X.shape[0], len(self.base_models))), 1)  \n",
    "            print('Using features of shape', S_test.shape)\n",
    "        else:\n",
    "            S_test = np.zeros((X.shape[0], len(self.base_models)))\n",
    "            print('Using features of shape', S_test.shape)\n",
    "\n",
    "        for ind, c in enumerate(self.base_models):\n",
    "            clf = self.base_models[c]\n",
    "            \n",
    "            # Uses all features.\n",
    "            if self.features:\n",
    "                i = X.shape[1] + ind\n",
    "            else:\n",
    "                i = ind\n",
    "                \n",
    "            S_test_i = np.zeros((X.shape[0], len(folds)))\n",
    "            print('--- Predicting For  #{}'.format(c))\n",
    "            \n",
    "            # Makes predictions for each model\n",
    "            for j, (train_idx, test_idx) in enumerate(folds):    \n",
    "                if c not in ['XGB', 'LSTM']:\n",
    "                    S_test_i[:, j] = clf.predict(X)[:]\n",
    "                    \n",
    "                elif c in ['LSTM']:\n",
    "                    S_test_i[:, j] = [i for i in clf.predict(X.reshape((X.shape[0], 1, X.shape[1])))[:]]\n",
    "                    \n",
    "                else:\n",
    "                    S_test_i[:, j] = clf.predict(X)[:]\n",
    "                \n",
    "            S_test[:, i] = S_test_i.mean(1)\n",
    "\n",
    "        clf = self.stacker\n",
    "        y_pred = clf.predict(S_test)[:]\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    \n",
    "    def fit_predict(self, X, y, T):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        T = np.array(T)\n",
    "\n",
    "        start_time = time.time()\n",
    "        folds = list(KFold(len(y), n_folds=self.n_folds, shuffle=True, random_state=2016))\n",
    "\n",
    "        S_train = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        S_test  = np.zeros((T.shape[0], len(self.base_models)))\n",
    "\n",
    "        for i, c in enumerate(self.base_models):\n",
    "            print('########## \\nFitting For Base Model {} \\n##########'.format(c))\n",
    "            clf = self.base_models[c]\n",
    "            S_test_i = np.zeros((T.shape[0], len(folds)))\n",
    "\n",
    "            for j, (train_idx, test_idx) in enumerate(folds):\n",
    "                print('--- Fitting For Fold #{0} / {1} ---'.format(j+1, self.n_folds))\n",
    "                X_train = X[train_idx]\n",
    "                y_train = y[train_idx]\n",
    "                X_holdout = X[test_idx]\n",
    "                \n",
    "                if c not in ['XGB', 'LightGBM', 'LSTM']:\n",
    "                    clf.fit(X_train, y_train)\n",
    "                    y_pred = clf.predict(X_holdout)[:]\n",
    "                    \n",
    "                    S_train[test_idx, i] = y_pred\n",
    "                    S_test_i[:, j] = clf.predict(T)[:]\n",
    "                    \n",
    "                elif c in ['LSTM']:\n",
    "                    x_train_lstm = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "                    \n",
    "                    clf.fit(x_train_lstm, y_train, epochs=15, batch_size=50, verbose=1, shuffle=False)\n",
    "                    y_pred = clf.predict(X_holdout.reshape((X_holdout.shape[0], 1, X_holdout.shape[1])))[:]\n",
    "                    \n",
    "                    S_train[test_idx, i] = [i[0] for i in y_pred]\n",
    "                    S_test_i[:, j] = [i for i in clf.predict(T.reshape((T.shape[0], 1, T.shape[1])))[:]]\n",
    "                    \n",
    "                else:\n",
    "                    d_train = xg.DMatrix(X_train, label=y_train, missing=-1)\n",
    "                    d_valid = xg.DMatrix(X_holdout, missing=-1)\n",
    "                    \n",
    "                    clf = xg.train(params_xgb, d_train)\n",
    "                    y_pred = clf.predict(d_valid)[:]\n",
    "                    \n",
    "                    S_train[test_idx, i] = y_pred\n",
    "                    data_pred = xg.DMatrix(T, missing=-1)\n",
    "                    S_test_i[:, j] = clf.predict(data_pred)[:]\n",
    "\n",
    "                print('Elapsed: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "            S_test[:, i] = S_test_i.mean(1)\n",
    "            print('Elapsed: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "        print('--- Base Models Trained: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "        param_grid = {'n_estimators':  [100],\n",
    "                      'learning_rate': [0.05],\n",
    "                      'subsample':     [0.75]}\n",
    "\n",
    "        grid = grid_search.GridSearchCV(estimator=self.stacker, param_grid=param_grid, n_jobs=1, cv=5, verbose=20, scoring=MAE)\n",
    "        grid.fit(S_train, y)\n",
    "\n",
    "        try:\n",
    "            print('Param grid:')\n",
    "            print(param_grid)\n",
    "            print('Best Params:')\n",
    "            print(grid.best_params_)\n",
    "            print('Best CV Score:')\n",
    "            print(-grid.best_score_)\n",
    "            print('Best estimator:')\n",
    "            print(grid.best_estimator_)\n",
    "            print(message)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        print('--- Stacker Trained: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "        y_pred = grid.predict(S_test)[:]\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del models['CatBoost']\n",
    "#del models['XGB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators':  [50, 100, 150],\n",
    "              'learning_rate': [0.03, 0.05, 0.07],\n",
    "              'subsample':     [0.5, 0.75, 1]\n",
    "             }\n",
    "\n",
    "ensemble = Ensemble(n_folds=5,\n",
    "                    stacker=GradientBoostingRegressor(random_state=2016, verbose=1),\n",
    "                    base_models=models, include_features=False, cvgrid=param_grid)\n",
    "                    \n",
    "#model_ensemble = ensemble.fit_predict(x_train[:100], y_train[:100], x_valid)\n",
    "# MAE 0.0653212760898 - lr 0.03, nest = 50, subsample: 0.5\n",
    "ensemble.fit(x_train, y_train)\n",
    "#final_prediction = ensemble.predict(x_valid)\n",
    "#print('MAE', mean_absolute_error(y_valid, final_prediction))\n",
    "\n",
    "#del final_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(X, K, randomise = False):\n",
    "    \"\"\"Generates K (training, validation) pairs from the items in X.\"\"\"\n",
    "    \n",
    "    if randomise: from random import shuffle; X=list(X); shuffle(X)\n",
    "    for k in range(K):\n",
    "        training   = [x for i, x in enumerate(X) if i % K != k]\n",
    "        validation = [x for i, x in enumerate(X) if i % K == k]\n",
    "        \n",
    "        yield training, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "for training, validation in k_fold_cross_validation(x_train_data.values, K=5):\n",
    "    pred_k = ensemble.predict(validation)\n",
    "    print('MAE',mean_absolute_error(x_train_label[index:index+len(validation)], pred_k))\n",
    "    index += len(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########## LAYER 1 ##########\n",
    "# Submodel  1 : OLS                      # Ordinary least squares estimator Sklearn implementation\n",
    "# Submodel  2 : BR                       # Bayesian ridge regression - Sklearn implementation\n",
    "# Submodel  3 : DNN                      # Dense Neural Network - Keras - Dense layers \n",
    "# Submodel  4 : LightGBM                 # Light Gradient Boosting - https://github.com/Microsoft/LightGBM\n",
    "# Submodel  5 : XGBoost                  # Extreme Gradient Boosting - http://xgboost.readthedocs.io/en/latest/model.html\n",
    "# Submodel  6 : CatBoost                 # Categorical Boosting https://github.com/catboost/catboost\n",
    "# Submodel  7 : LSTM                     # Long Short Term Memory Neural Network - Keras implementation\n",
    "# Submodel  8 : RandomForestRegressor    # Sklearn implementation\n",
    "# Submodel  9 : ExtraTreesRegressor      # Sklearn implementation\n",
    "# Submodel 10 : SVR                      # Support vector machines for regression - Sklearn implementation\n",
    "# Submodel 11 : AdaBoost                 # Adaptive Boosting Sklearn Implementation\n",
    "\n",
    "########## LAYER 2 ##########\n",
    "# https://www.kaggle.com/dragost/boosted-trees-lb-0-0643707/edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_predict = transactions.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_sample[m] = submission_sample['ParcelId'].to_frame().merge(x_predict[['parcelid', m]], how='left', left_on='ParcelId', right_on='parcelid')[m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/zillow-prize-1/discussion/33899, Oct,Nov,Dec\n",
    "test_dates = {\n",
    "    '201610': pd.Timestamp('2016-09-30'),\n",
    "    '201611': pd.Timestamp('2016-10-31'),\n",
    "    '201612': pd.Timestamp('2016-11-30'),\n",
    "    '201710': pd.Timestamp('2017-09-30'),\n",
    "    '201711': pd.Timestamp('2017-10-31'),\n",
    "    '201712': pd.Timestamp('2017-11-30')\n",
    "}\n",
    "\n",
    "all_preds = pd.DataFrame()\n",
    "for m in test_dates.keys():\n",
    "    # Building predictions.\n",
    "    print('Processing', m)\n",
    "    x_predict = transactions.copy()\n",
    "    x_predict = complex_features(x_predict)\n",
    "    x_predict['transactiondate'] = test_dates[m]\n",
    "    x_predict = time_data(x_predict)\n",
    "    \n",
    "    print('Cleaning data')\n",
    "    print('........')\n",
    "    \n",
    "    x_predict = x_predict.fillna(-999).astype(str).apply(LabelEncoder().fit_transform)\n",
    "    \n",
    "    print('Predicting')\n",
    "    print('........')\n",
    "    \n",
    "    x_predict[m] = xgb_gs2.predict(xg.DMatrix(x_predict[best_columns]))\n",
    "    all_preds[m] = x_predict[m].copy()\n",
    "    submission_sample[m] = submission_sample['ParcelId'].to_frame().merge(x_predict[['parcelid', m]], how='left', left_on='ParcelId', right_on='parcelid')[m]\n",
    "    \n",
    "#del x_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_predict[list(test_dates.keys())] = all_preds[list(test_dates.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = '201610'\n",
    "submission_sample[m] = submission_sample['ParcelId'].to_frame().merge(x_predict[['parcelid', m]], how='left', left_on='ParcelId', right_on='parcelid')[m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_sample.to_csv('submission4.csv',index=False)\n",
    "submission_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/zillow-prize-1/discussion/33899, Oct,Nov,Dec\n",
    "test_dates = {'201610': pd.Timestamp('2016-09-30'),\n",
    "              '201611': pd.Timestamp('2016-10-31'),\n",
    "              '201612': pd.Timestamp('2016-11-30')}\n",
    "\n",
    "x_predict = transactions.copy()\n",
    "x_predict = complex_features(x_predict)\n",
    "\n",
    "for m in test_dates.keys():\n",
    "    print('Processing', m)  \n",
    "    x_predict['transactiondate'] = test_dates[m]\n",
    "    x_predict = time_data(x_predict)\n",
    "    x_predict = x_predict[best_columns].fillna(-999).astype(str).apply(LabelEncoder().fit_transform)\n",
    "    \n",
    "    print('Predicting')\n",
    "    print('........')\n",
    "    \n",
    "    # 5 iterations cat booster for each prediction.\n",
    "    x_predict[m] = cat_booster(x_train, y_train, x_predict.values)\n",
    "    submission_sample[m] = submission_sample['ParcelId'].to_frame().merge(x_predict[['parcelid', m]], how='left', left_on='ParcelId', right_on='parcelid')[m]\n",
    "    \n",
    "del x_predict\n",
    "del predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RFE"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#model_lr = LinearRegression()\n",
    "model_xgb = xg.XGBRegressor()\n",
    "selector = RFE(model_xgb, 100, step=500)\n",
    "model = selector.estimator.fit(x_train, y_train)\n",
    "\n",
    "dict_features = plot_best_features(model, data=x_all, num_features=100, figsize=(5,15))\n",
    "\n",
    "best_columns = list(dict_features.keys())\n",
    "#new_sparse_columns = x_all.columns\n",
    "x_train = pd.DataFrame(x_train, columns=x_all.columns)[best_columns].values\n",
    "x_test  = pd.DataFrame(x_test,  columns=x_all.columns)[best_columns].values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
