{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading data from disk ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/dragost/boosted-trees-lb-0-0643707/edit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import random\n",
    "import datetime as dt\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from tqdm import tqdm\n",
    "\n",
    "################\n",
    "################\n",
    "##  LightGBM changes ##\n",
    "# V42 - sub_feature: 0.3 -> 0.35 : LB = 0.0643759\n",
    "# V34 - sub_feature: 0.5 -> 0.42\n",
    "# V33 - sub_feature: 0.5 -> 0.45 : LB = 0.0643866\n",
    "# - sub_feature: 0.45 -> 0.3 : LB = 0.0643811 / 0.0643814 \n",
    "################\n",
    "################ \n",
    "\n",
    "##### READ IN RAW DATA\n",
    "print( \"\\nReading data from disk ...\")\n",
    "prop = pd.read_csv('../Data/properties_2017.csv')\n",
    "train = pd.read_csv(\"../Data/train_2017.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "XGB_WEIGHT = 0.6500\n",
    "BASELINE_WEIGHT = 0.0056\n",
    "OLS_WEIGHT = 0.0828\n",
    "\n",
    "XGB1_WEIGHT = 0.8083  # Weight of first in combination of two XGB models\n",
    "\n",
    "BASELINE_PRED = 0.0115   # Baseline based on mean of training data, per Oleg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing data for LightGBM ...\n",
      "(77613, 53) (77613,)\n",
      "\n",
      "Fitting LightGBM model ...\n",
      "\n",
      "Prepare for LightGBM prediction ...\n",
      "   Read sample file ...\n",
      "   ...\n",
      "   Merge with property data ...\n",
      "   ...\n",
      "   ...\n",
      "   ...\n",
      "   Preparing x_test...\n",
      "   ...\n",
      "Test shape : (2985217, 53)\n",
      "\n",
      "Start LightGBM prediction ...\n",
      "\n",
      "Unadjusted LightGBM predictions:\n",
      "          0\n",
      "0  0.025868\n",
      "1  0.026412\n",
      "2  0.020930\n",
      "3  0.027978\n",
      "4  0.020549\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "################\n",
    "##  LightGBM  ##\n",
    "################\n",
    "################\n",
    "\n",
    "##### PROCESS DATA FOR LIGHTGBM\n",
    "print( \"\\nProcessing data for LightGBM ...\" )\n",
    "for c, dtype in zip(prop.columns, prop.dtypes):\t\n",
    "    if dtype == np.float64:\n",
    "        prop[c] = prop[c].astype(np.float32)\n",
    "\n",
    "df_train = train.merge(prop, how='left', on='parcelid')\n",
    "df_train.fillna(df_train.median(),inplace = True)\n",
    "\n",
    "x_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', \n",
    "                         'propertycountylandusecode', 'fireplacecnt', 'fireplaceflag'], axis=1)\n",
    "#x_train['Ratio_1'] = x_train['taxvaluedollarcnt']/x_train['taxamount']\n",
    "y_train = df_train['logerror'].values\n",
    "print(x_train.shape, y_train.shape)\n",
    "\n",
    "\n",
    "train_columns = x_train.columns\n",
    "\n",
    "for c in x_train.dtypes[x_train.dtypes == object].index.values:\n",
    "    x_train[c] = (x_train[c] == True)\n",
    "\n",
    "del df_train; gc.collect()\n",
    "\n",
    "x_train = x_train.values.astype(np.float32, copy=False)\n",
    "d_train = lgb.Dataset(x_train, label=y_train)\n",
    "\n",
    "\n",
    "\n",
    "##### RUN LIGHTGBM\n",
    "params = {}\n",
    "params['max_bin'] = 10\n",
    "params['learning_rate'] = 0.0021 # shrinkage_rate\n",
    "params['boosting_type'] = 'gbdt'\n",
    "params['objective'] = 'regression'\n",
    "params['metric'] = 'l1'          # or 'mae'\n",
    "params['sub_feature'] = 0.345    \n",
    "params['bagging_fraction'] = 0.85 # sub_row\n",
    "params['bagging_freq'] = 40\n",
    "params['num_leaves'] = 512        # num_leaf\n",
    "params['min_data'] = 500         # min_data_in_leaf\n",
    "params['min_hessian'] = 0.05     # min_sum_hessian_in_leaf\n",
    "params['verbose'] = 0\n",
    "#params['feature_fraction_seed'] = 2\n",
    "#params['bagging_seed'] = 3\n",
    "\n",
    "print(\"\\nFitting LightGBM model ...\")\n",
    "clf = lgb.train(params, d_train, 430)\n",
    "\n",
    "del d_train; gc.collect()\n",
    "del x_train; gc.collect()\n",
    "\n",
    "print(\"\\nPrepare for LightGBM prediction ...\")\n",
    "print(\"   Read sample file ...\")\n",
    "sample = pd.read_csv('../Data/sample_submission.csv')\n",
    "print(\"   ...\")\n",
    "sample['parcelid'] = sample['ParcelId']\n",
    "print(\"   Merge with property data ...\")\n",
    "df_test = sample.merge(prop, on='parcelid', how='left')\n",
    "print(\"   ...\")\n",
    "del sample, prop; gc.collect()\n",
    "print(\"   ...\")\n",
    "#df_test['Ratio_1'] = df_test['taxvaluedollarcnt']/df_test['taxamount']\n",
    "x_test = df_test[train_columns]\n",
    "print(\"   ...\")\n",
    "del df_test; gc.collect()\n",
    "print(\"   Preparing x_test...\")\n",
    "for c in x_test.dtypes[x_test.dtypes == object].index.values:\n",
    "    x_test[c] = (x_test[c] == True)\n",
    "print(\"   ...\")\n",
    "x_test = x_test.values.astype(np.float32, copy=False)\n",
    "print(\"Test shape :\", x_test.shape)\n",
    "\n",
    "print(\"\\nStart LightGBM prediction ...\")\n",
    "p_test = clf.predict(x_test)\n",
    "\n",
    "del x_test; gc.collect()\n",
    "\n",
    "print( \"\\nUnadjusted LightGBM predictions:\" )\n",
    "print( pd.DataFrame(p_test).head() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Re-reading properties file ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing data for XGBoost ...\n",
      "Shape train: (77613, 57)\n",
      "Shape test: (2985217, 57)\n",
      "After removing outliers:\n",
      "Shape train: (75949, 57)\n",
      "Shape test: (2985217, 57)\n",
      "\n",
      "Setting up data for XGBoost ...\n",
      "num_boost_rounds=250\n",
      "\n",
      "Training XGBoost ...\n",
      "\n",
      "Predicting with XGBoost ...\n",
      "\n",
      "First XGBoost predictions:\n",
      "          0\n",
      "0  0.064053\n",
      "1  0.035960\n",
      "2 -0.075041\n",
      "3 -0.003773\n",
      "4 -0.001748\n",
      "\n",
      "Setting up data for XGBoost ...\n",
      "num_boost_rounds=150\n",
      "\n",
      "Training XGBoost again ...\n",
      "\n",
      "Predicting with XGBoost again ...\n",
      "\n",
      "Second XGBoost predictions:\n",
      "          0\n",
      "0  0.052026\n",
      "1  0.041535\n",
      "2 -0.086148\n",
      "3 -0.024098\n",
      "4 -0.000185\n",
      "\n",
      "Combined XGBoost predictions:\n",
      "          0\n",
      "0  0.061747\n",
      "1  0.037029\n",
      "2 -0.077170\n",
      "3 -0.007669\n",
      "4 -0.001449\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################\n",
    "################\n",
    "##  XGBoost   ##\n",
    "################\n",
    "################\n",
    "\n",
    "##### RE-READ PROPERTIES FILE\n",
    "##### (I tried keeping a copy, but the program crashed.)\n",
    "\n",
    "print( \"\\nRe-reading properties file ...\")\n",
    "properties = pd.read_csv('../Data/properties_2017.csv')\n",
    "\n",
    "##### PROCESS DATA FOR XGBOOST\n",
    "print( \"\\nProcessing data for XGBoost ...\")\n",
    "for c in properties.columns:\n",
    "    properties[c]=properties[c].fillna(-1)\n",
    "    if properties[c].dtype == 'object':\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(properties[c].values))\n",
    "        properties[c] = lbl.transform(list(properties[c].values))\n",
    "\n",
    "train_df = train.merge(properties, how='left', on='parcelid')\n",
    "x_train = train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\n",
    "x_test = properties.drop(['parcelid'], axis=1)\n",
    "# shape        \n",
    "print('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))\n",
    "\n",
    "# drop out ouliers\n",
    "train_df=train_df[ train_df.logerror > -0.4 ]\n",
    "train_df=train_df[ train_df.logerror < 0.419 ]\n",
    "x_train=train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\n",
    "y_train = train_df[\"logerror\"].values.astype(np.float32)\n",
    "y_mean = np.mean(y_train)\n",
    "\n",
    "print('After removing outliers:')     \n",
    "print('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### RUN XGBOOST\n",
    "print(\"\\nSetting up data for XGBoost ...\")\n",
    "# xgboost params\n",
    "xgb_params = {\n",
    "    'eta': 0.037,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.80,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'mae',\n",
    "    'lambda': 0.8,   \n",
    "    'alpha': 0.4, \n",
    "    'base_score': y_mean,\n",
    "    'silent': 1\n",
    "}\n",
    "\n",
    "dtrain = xgb.DMatrix(x_train, y_train)\n",
    "dtest = xgb.DMatrix(x_test)\n",
    "\n",
    "num_boost_rounds = 250\n",
    "print(\"num_boost_rounds=\"+str(num_boost_rounds))\n",
    "\n",
    "# train model\n",
    "print( \"\\nTraining XGBoost ...\")\n",
    "model = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n",
    "\n",
    "print( \"\\nPredicting with XGBoost ...\")\n",
    "xgb_pred1 = model.predict(dtest)\n",
    "\n",
    "print( \"\\nFirst XGBoost predictions:\" )\n",
    "print( pd.DataFrame(xgb_pred1).head() )\n",
    "\n",
    "\n",
    "\n",
    "##### RUN XGBOOST AGAIN\n",
    "print(\"\\nSetting up data for XGBoost ...\")\n",
    "# xgboost params\n",
    "xgb_params = {\n",
    "    'eta': 0.033,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.80,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'mae',\n",
    "    'base_score': y_mean,\n",
    "    'silent': 1\n",
    "}\n",
    "\n",
    "num_boost_rounds = 150\n",
    "print(\"num_boost_rounds=\"+str(num_boost_rounds))\n",
    "\n",
    "print( \"\\nTraining XGBoost again ...\")\n",
    "model = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n",
    "\n",
    "print( \"\\nPredicting with XGBoost again ...\")\n",
    "xgb_pred2 = model.predict(dtest)\n",
    "\n",
    "print( \"\\nSecond XGBoost predictions:\" )\n",
    "print( pd.DataFrame(xgb_pred2).head() )\n",
    "\n",
    "\n",
    "\n",
    "##### COMBINE XGBOOST RESULTS\n",
    "\n",
    "xgb_pred = XGB1_WEIGHT*xgb_pred1 + (1-XGB1_WEIGHT)*xgb_pred2\n",
    "#xgb_pred = xgb_pred1\n",
    "\n",
    "print( \"\\nCombined XGBoost predictions:\" )\n",
    "print( pd.DataFrame(xgb_pred).head() )\n",
    "\n",
    "del train_df\n",
    "del x_train\n",
    "del x_test\n",
    "del properties\n",
    "del dtest\n",
    "del dtrain\n",
    "del xgb_pred1\n",
    "del xgb_pred2 \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77613 2985217 2985217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit...\n",
      "0.0709592335615\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "################\n",
    "##    OLS     ##\n",
    "################\n",
    "################\n",
    "\n",
    "# This section is derived from the1owl's notebook:\n",
    "#    https://www.kaggle.com/the1owl/primer-for-the-zillow-pred-approach\n",
    "# which I (Andy Harless) updated and made into a script:\n",
    "#    https://www.kaggle.com/aharless/updated-script-version-of-the1owl-s-basic-ols\n",
    "np.random.seed(17)\n",
    "random.seed(17)\n",
    "\n",
    "train = pd.read_csv(\"../Data/train_2017.csv\", parse_dates=[\"transactiondate\"])\n",
    "properties = pd.read_csv(\"../Data/properties_2017.csv\")\n",
    "submission = pd.read_csv(\"../Data/sample_submission.csv\")\n",
    "print(len(train),len(properties),len(submission))\n",
    "\n",
    "def get_features(df):\n",
    "    df[\"transactiondate\"] = pd.to_datetime(df[\"transactiondate\"])\n",
    "    df[\"transactiondate_year\"] = df[\"transactiondate\"].dt.year\n",
    "    df[\"transactiondate_month\"] = df[\"transactiondate\"].dt.month\n",
    "    df['transactiondate'] = df['transactiondate'].dt.quarter\n",
    "    df = df.fillna(-1.0)\n",
    "    return df\n",
    "\n",
    "def MAE(y, ypred):\n",
    "    #logerror=log(Zestimate)−log(SalePrice)\n",
    "    return np.sum([abs(y[i]-ypred[i]) for i in range(len(y))]) / len(y)\n",
    "\n",
    "train = pd.merge(train, properties, how='left', on='parcelid')\n",
    "y = train['logerror'].values\n",
    "test = pd.merge(submission, properties, how='left', left_on='ParcelId', right_on='parcelid')\n",
    "properties = [] #memory\n",
    "\n",
    "exc = [train.columns[c] for c in range(len(train.columns)) if train.dtypes[c] == 'O'] + ['logerror','parcelid']\n",
    "col = [c for c in train.columns if c not in exc]\n",
    "\n",
    "train = get_features(train[col])\n",
    "test['transactiondate'] = '2016-01-01' #should use the most common training date\n",
    "test = get_features(test[col])\n",
    "\n",
    "reg = LinearRegression(n_jobs=-1)\n",
    "reg.fit(train, y); print('fit...')\n",
    "print(MAE(y, reg.predict(train)))\n",
    "train = [];  y = [] #memory\n",
    "\n",
    "test_dates = ['2016-10-01','2016-11-01','2016-12-01','2017-10-01','2017-11-01','2017-12-01']\n",
    "test_columns = ['201610','201611','201612','201710','201711','201712']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Processing data for Neural Network ...\n",
      "\n",
      "Loading train, prop and sample data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Label Encoder on properties...\n",
      "Creating training set...\n",
      "Filling NA/NaN values...\n",
      "Creating x_train and y_train from df_train...\n",
      "(77613, 56) (77613,)\n",
      "Creating df_test...\n",
      "Merging Sample with property data...\n",
      "Shape of x_test: (2985217, 56)\n",
      "Preparing x_test...\n",
      "\n",
      "Preprocessing neural network data...\n",
      "len_x is: 56\n",
      "\n",
      "Setting up neural network model...\n",
      "\n",
      "Fitting neural network model...\n",
      "Epoch 1/70\n",
      "17s - loss: 0.0731\n",
      "Epoch 2/70\n",
      "15s - loss: 0.0698\n",
      "Epoch 3/70\n",
      "14s - loss: 0.0697\n",
      "Epoch 4/70\n",
      "14s - loss: 0.0697\n",
      "Epoch 5/70\n",
      "15s - loss: 0.0697\n",
      "Epoch 6/70\n",
      "17s - loss: 0.0696\n",
      "Epoch 7/70\n",
      "17s - loss: 0.0696\n",
      "Epoch 8/70\n",
      "17s - loss: 0.0696\n",
      "Epoch 9/70\n",
      "18s - loss: 0.0695\n",
      "Epoch 10/70\n",
      "18s - loss: 0.0695\n",
      "Epoch 11/70\n",
      "18s - loss: 0.0695\n",
      "Epoch 12/70\n",
      "17s - loss: 0.0694\n",
      "Epoch 13/70\n",
      "18s - loss: 0.0694\n",
      "Epoch 14/70\n",
      "18s - loss: 0.0694\n",
      "Epoch 15/70\n",
      "17s - loss: 0.0694\n",
      "Epoch 16/70\n",
      "16s - loss: 0.0693\n",
      "Epoch 17/70\n",
      "17s - loss: 0.0693\n",
      "Epoch 18/70\n",
      "16s - loss: 0.0693\n",
      "Epoch 19/70\n",
      "16s - loss: 0.0693\n",
      "Epoch 20/70\n",
      "17s - loss: 0.0693\n",
      "Epoch 21/70\n",
      "17s - loss: 0.0693\n",
      "Epoch 22/70\n",
      "18s - loss: 0.0693\n",
      "Epoch 23/70\n",
      "18s - loss: 0.0692\n",
      "Epoch 24/70\n",
      "18s - loss: 0.0693\n",
      "Epoch 25/70\n",
      "17s - loss: 0.0693\n",
      "Epoch 26/70\n",
      "19s - loss: 0.0693\n",
      "Epoch 27/70\n",
      "23s - loss: 0.0692\n",
      "Epoch 28/70\n",
      "24s - loss: 0.0692\n",
      "Epoch 29/70\n",
      "21s - loss: 0.0692\n",
      "Epoch 30/70\n",
      "21s - loss: 0.0692\n",
      "Epoch 31/70\n",
      "24s - loss: 0.0692\n",
      "Epoch 32/70\n",
      "23s - loss: 0.0692\n",
      "Epoch 33/70\n",
      "22s - loss: 0.0692\n",
      "Epoch 34/70\n",
      "24s - loss: 0.0692\n",
      "Epoch 35/70\n",
      "20s - loss: 0.0692\n",
      "Epoch 36/70\n",
      "23s - loss: 0.0691\n",
      "Epoch 37/70\n",
      "19s - loss: 0.0692\n",
      "Epoch 38/70\n",
      "17s - loss: 0.0691\n",
      "Epoch 39/70\n",
      "18s - loss: 0.0692\n",
      "Epoch 40/70\n",
      "18s - loss: 0.0692\n",
      "Epoch 41/70\n",
      "16s - loss: 0.0691\n",
      "Epoch 42/70\n",
      "16s - loss: 0.0691\n",
      "Epoch 43/70\n",
      "16s - loss: 0.0691\n",
      "Epoch 44/70\n",
      "16s - loss: 0.0691\n",
      "Epoch 45/70\n",
      "19s - loss: 0.0691\n",
      "Epoch 46/70\n",
      "22s - loss: 0.0691\n",
      "Epoch 47/70\n",
      "24s - loss: 0.0691\n",
      "Epoch 48/70\n",
      "22s - loss: 0.0691\n",
      "Epoch 49/70\n",
      "20s - loss: 0.0691\n",
      "Epoch 50/70\n",
      "21s - loss: 0.0691\n",
      "Epoch 51/70\n",
      "23s - loss: 0.0691\n",
      "Epoch 52/70\n",
      "22s - loss: 0.0691\n",
      "Epoch 53/70\n",
      "22s - loss: 0.0691\n",
      "Epoch 54/70\n",
      "21s - loss: 0.0691\n",
      "Epoch 55/70\n",
      "20s - loss: 0.0691\n",
      "Epoch 56/70\n",
      "20s - loss: 0.0691\n",
      "Epoch 57/70\n",
      "18s - loss: 0.0691\n",
      "Epoch 58/70\n",
      "17s - loss: 0.0691\n",
      "Epoch 59/70\n",
      "17s - loss: 0.0690\n",
      "Epoch 60/70\n",
      "18s - loss: 0.0690\n",
      "Epoch 61/70\n",
      "17s - loss: 0.0691\n",
      "Epoch 62/70\n",
      "18s - loss: 0.0690\n",
      "Epoch 63/70\n",
      "17s - loss: 0.0690\n",
      "Epoch 64/70\n",
      "16s - loss: 0.0690\n",
      "Epoch 65/70\n",
      "17s - loss: 0.0690\n",
      "Epoch 66/70\n",
      "17s - loss: 0.0690\n",
      "Epoch 67/70\n",
      "18s - loss: 0.0690\n",
      "Epoch 68/70\n",
      "26s - loss: 0.0690\n",
      "Epoch 69/70\n",
      "21s - loss: 0.0690\n",
      "Epoch 70/70\n",
      "28s - loss: 0.0690\n",
      "\n",
      "Predicting with neural network model...\n",
      "\n",
      "Preparing results for write...\n",
      "Type of nn_pred is  <class 'numpy.ndarray'>\n",
      "Shape of nn_pred is  (2985217,)\n",
      "\n",
      "Neural Network predictions:\n",
      "          0\n",
      "0  0.011157\n",
      "1  0.008576\n",
      "2  0.326974\n",
      "3 -0.092062\n",
      "4 -0.074120\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout, BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.noise import GaussianDropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "\n",
    "######################\n",
    "######################\n",
    "##  Neural Network  ##\n",
    "######################\n",
    "######################\n",
    "\n",
    "# Neural network copied from this script:\n",
    "#   https://www.kaggle.com/aharless/keras-neural-network-lb-06492 (version 20)\n",
    "# which was built on the skeleton in this notebook:\n",
    "#   https://www.kaggle.com/prasunmishra/ann-using-keras\n",
    "\n",
    "\n",
    "# Read in data for neural network\n",
    "print( \"\\n\\nProcessing data for Neural Network ...\")\n",
    "print('\\nLoading train, prop and sample data...')\n",
    "train = pd.read_csv(\"../Data/train_2017.csv\", parse_dates=[\"transactiondate\"])\n",
    "prop = pd.read_csv('../Data/properties_2017.csv')\n",
    "sample = pd.read_csv('../Data/sample_submission.csv')\n",
    " \n",
    "print('Fitting Label Encoder on properties...')\n",
    "for c in prop.columns:\n",
    "    prop[c]=prop[c].fillna(-1)\n",
    "    if prop[c].dtype == 'object':\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(prop[c].values))\n",
    "        prop[c] = lbl.transform(list(prop[c].values))\n",
    "        \n",
    "print('Creating training set...')\n",
    "df_train = train.merge(prop, how='left', on='parcelid')\n",
    "\n",
    "df_train[\"transactiondate\"] = pd.to_datetime(df_train[\"transactiondate\"])\n",
    "df_train[\"transactiondate_year\"] = df_train[\"transactiondate\"].dt.year\n",
    "df_train[\"transactiondate_month\"] = df_train[\"transactiondate\"].dt.month\n",
    "df_train['transactiondate_quarter'] = df_train['transactiondate'].dt.quarter\n",
    "df_train[\"transactiondate\"] = df_train[\"transactiondate\"].dt.day\n",
    "\n",
    "print('Filling NA/NaN values...' )\n",
    "df_train.fillna(-1.0)\n",
    "\n",
    "print('Creating x_train and y_train from df_train...' )\n",
    "x_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', 'propertycountylandusecode','fireplacecnt', 'fireplaceflag'], axis=1)\n",
    "y_train = df_train[\"logerror\"]\n",
    "\n",
    "y_mean = np.mean(y_train)\n",
    "print(x_train.shape, y_train.shape)\n",
    "train_columns = x_train.columns\n",
    "\n",
    "for c in x_train.dtypes[x_train.dtypes == object].index.values:\n",
    "    x_train[c] = (x_train[c] == True)\n",
    "\n",
    "print('Creating df_test...')\n",
    "sample['parcelid'] = sample['ParcelId']\n",
    "\n",
    "print(\"Merging Sample with property data...\")\n",
    "df_test = sample.merge(prop, on='parcelid', how='left')\n",
    "\n",
    "df_test[\"transactiondate\"] = pd.to_datetime('2016-11-15')  # placeholder value for preliminary version\n",
    "df_test[\"transactiondate_year\"] = df_test[\"transactiondate\"].dt.year\n",
    "df_test[\"transactiondate_month\"] = df_test[\"transactiondate\"].dt.month\n",
    "df_test['transactiondate_quarter'] = df_test['transactiondate'].dt.quarter\n",
    "df_test[\"transactiondate\"] = df_test[\"transactiondate\"].dt.day     \n",
    "x_test = df_test[train_columns]\n",
    "\n",
    "print('Shape of x_test:', x_test.shape)\n",
    "print(\"Preparing x_test...\")\n",
    "for c in x_test.dtypes[x_test.dtypes == object].index.values:\n",
    "    x_test[c] = (x_test[c] == True)\n",
    "\n",
    "    \n",
    "## Preprocessing\n",
    "print(\"\\nPreprocessing neural network data...\")\n",
    "imputer= Imputer()\n",
    "imputer.fit(x_train.iloc[:, :])\n",
    "x_train = imputer.transform(x_train.iloc[:, :])\n",
    "imputer.fit(x_test.iloc[:, :])\n",
    "x_test = imputer.transform(x_test.iloc[:, :])\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "\n",
    "len_x=int(x_train.shape[1])\n",
    "print(\"len_x is:\",len_x)\n",
    "\n",
    "\n",
    "# Neural Network\n",
    "print(\"\\nSetting up neural network model...\")\n",
    "nn = Sequential()\n",
    "nn.add(Dense(units = 400 , kernel_initializer = 'normal', input_dim = len_x))\n",
    "nn.add(PReLU())\n",
    "nn.add(Dropout(.4))\n",
    "nn.add(Dense(units = 160 , kernel_initializer = 'normal'))\n",
    "nn.add(PReLU())\n",
    "nn.add(BatchNormalization())\n",
    "nn.add(Dropout(.6))\n",
    "nn.add(Dense(units = 64 , kernel_initializer = 'normal'))\n",
    "nn.add(PReLU())\n",
    "nn.add(BatchNormalization())\n",
    "nn.add(Dropout(.5))\n",
    "nn.add(Dense(units = 26, kernel_initializer = 'normal'))\n",
    "nn.add(PReLU())\n",
    "nn.add(BatchNormalization())\n",
    "nn.add(Dropout(.6))\n",
    "nn.add(Dense(1, kernel_initializer='normal'))\n",
    "nn.compile(loss='mae', optimizer=Adam(lr=4e-3, decay=1e-4))\n",
    "\n",
    "print(\"\\nFitting neural network model...\")\n",
    "nn.fit(np.array(x_train), np.array(y_train), batch_size = 32, epochs = 70, verbose=2)\n",
    "\n",
    "print(\"\\nPredicting with neural network model...\")\n",
    "#print(\"x_test.shape:\",x_test.shape)\n",
    "y_pred_ann = nn.predict(x_test)\n",
    "\n",
    "print( \"\\nPreparing results for write...\" )\n",
    "nn_pred = y_pred_ann.flatten()\n",
    "print( \"Type of nn_pred is \", type(nn_pred) )\n",
    "print( \"Shape of nn_pred is \", nn_pred.shape )\n",
    "\n",
    "print( \"\\nNeural Network predictions:\" )\n",
    "print( pd.DataFrame(nn_pred).head() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combining XGBoost, LightGBM, and baseline predicitons ...\n",
      "\n",
      "Combined XGB/LGB/baseline predictions:\n",
      "          0\n",
      "0  0.053542\n",
      "1  0.036229\n",
      "2 -0.046760\n",
      "3  0.005141\n",
      "4  0.006760\n",
      "\n",
      "Predicting with OLS and combining with XGB/LGB/baseline predicitons: ...\n",
      "predict... 0\n",
      "predict... 1\n",
      "predict... 2\n",
      "predict... 3\n",
      "predict... 4\n",
      "predict... 5\n",
      "\n",
      "Combined XGB/LGB/baseline/OLS predictions:\n",
      "   ParcelId  201610  201611  201612  201710  201711  201712\n",
      "0  10754147  0.0552  0.0552  0.0553  0.0552  0.0552  0.0553\n",
      "1  10759547  0.0389  0.0389  0.0389  0.0389  0.0389  0.0389\n",
      "2  10843547 -0.0141 -0.0141 -0.0141 -0.0141 -0.0141 -0.0141\n",
      "3  10859147  0.0085  0.0085  0.0086  0.0085  0.0085  0.0086\n",
      "4  10879947  0.0120  0.0120  0.0120  0.0120  0.0120  0.0120\n",
      "\n",
      "Writing results to disk ...\n",
      "\n",
      "Finished ...\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "########################\n",
    "##  Combine and Save  ##\n",
    "########################\n",
    "########################\n",
    "\n",
    "##### COMBINE PREDICTIONS\n",
    "print( \"\\nCombining XGBoost, LightGBM, and baseline predicitons ...\" )\n",
    "lgb_weight = (1 - XGB_WEIGHT - BASELINE_WEIGHT) / (1 - OLS_WEIGHT)\n",
    "xgb_weight0 = XGB_WEIGHT / (1 - OLS_WEIGHT)\n",
    "baseline_weight0 =  BASELINE_WEIGHT / (1 - OLS_WEIGHT)\n",
    "pred0 = xgb_weight0*xgb_pred + baseline_weight0*BASELINE_PRED + lgb_weight*p_test\n",
    "\n",
    "print( \"\\nCombined XGB/LGB/baseline predictions:\" )\n",
    "print( pd.DataFrame(pred0).head() )\n",
    "\n",
    "print( \"\\nPredicting with OLS and combining with XGB/LGB/baseline predicitons: ...\" )\n",
    "for i in range(len(test_dates)):\n",
    "    test['transactiondate'] = test_dates[i]\n",
    "    pred = OLS_WEIGHT*reg.predict(get_features(test)) + (1-OLS_WEIGHT)*pred0\n",
    "    submission[test_columns[i]] = [float(format(x, '.4f')) for x in pred]\n",
    "    print('predict...', i)\n",
    "\n",
    "print( \"\\nCombined XGB/LGB/baseline/OLS predictions:\" )\n",
    "print( submission.head() )\n",
    "\n",
    "\n",
    "\n",
    "##### WRITE THE RESULTS\n",
    "from datetime import datetime\n",
    "print( \"\\nWriting results to disk ...\" )\n",
    "submission.to_csv('sub{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False)\n",
    "print( \"\\nFinished ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "catboostx = pd.read_csv('Only_CatBoost.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_prediction = catboostx.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genetic = pd.read_csv('xxx.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories_x = ['201610','201611','201612','201710','201711','201712']\n",
    "new_prediction[categories_x] = genetic[categories_x] * 0.3 + (catboostx[categories_x]) * 0.5 + (submission[categories_x] * 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_prediction.to_csv('genetic_catboost_ensemble_3_5_2.csv', index=False, float_format='%.6f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_data(test, pred, sample, title, width=40, height=10, linewidth=0.5, color1='white', color2='orange'):\n",
    "    \"\"\" Plotting method. \"\"\"\n",
    "    fig = plt.figure(figsize=(width, height))\n",
    "    plt.plot(pred[:sample], color=color1, zorder=4, linewidth=linewidth, label='%s Prediction'%(title))\n",
    "    plt.plot(test[:sample], color=color2, zorder=3, linewidth=linewidth, label='%s True Data'%(title))\n",
    "    plt.title = title\n",
    "    plt.legend()\n",
    "\n",
    "# Frequency count\n",
    "def get_frequency(data):\n",
    "    # Gets the frequency of a column's values in 'data'. Pass on a series.\n",
    "    vals = pd.merge(data.to_frame(), data.value_counts().reset_index(), \n",
    "                    how='left', left_on=data.to_frame().columns[0], right_on='index').iloc[:, -1:].values\n",
    "    return vals\n",
    "  \n",
    "def time_data(data):\n",
    "    data['transactiondate'] = pd.to_datetime(data['transactiondate'])\n",
    "    data['day_of_week']     = data['transactiondate'].dt.dayofweek\n",
    "    data['month_of_year']   = data['transactiondate'].dt.month\n",
    "    data['quarter']         = data['transactiondate'].dt.quarter\n",
    "    data['is_weekend']      = (data['day_of_week'] < 5).astype(int)\n",
    "    data.drop('transactiondate', axis=1, inplace=True)\n",
    "    \n",
    "    print('Added time data')\n",
    "    print('........')\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def column_excluder(data, missing_perc_thresh=0.98):\n",
    "    # Quick clean from https://www.kaggle.com/seesee/concise-catboost-starter-ensemble-plb-0-06435\n",
    "    \n",
    "    exclude_missing = []\n",
    "    exclude_unique = []\n",
    "    num_rows = data.shape[0]\n",
    "    for c in data.columns:\n",
    "        num_missing = data[c].isnull().sum()\n",
    "        if num_missing == 0:\n",
    "            continue\n",
    "        missing_frac = num_missing / float(num_rows)\n",
    "        if missing_frac > missing_perc_thresh:\n",
    "            exclude_missing.append(c)\n",
    "\n",
    "        num_uniques = len(data[c].unique())\n",
    "        if data[c].isnull().sum() != 0:\n",
    "            num_uniques -= 1\n",
    "        if num_uniques == 1:\n",
    "            exclude_unique.append(c)\n",
    "            \n",
    "    to_exclude = list(set(exclude_missing + exclude_unique))\n",
    "    \n",
    "    print('Excluded columns:')\n",
    "    print(to_exclude)\n",
    "    print('........')\n",
    "    \n",
    "    return to_exclude\n",
    "\n",
    "def categorical_features(data):\n",
    "    # Quick categories from https://www.kaggle.com/seesee/concise-catboost-starter-ensemble-plb-0-06435\n",
    "        \n",
    "    cat_feature_inds = []\n",
    "    cat_unique_thresh = 1000\n",
    "    for i, c in enumerate(data.columns):\n",
    "        num_uniques = len(data[c].unique())\n",
    "        if num_uniques < cat_unique_thresh \\\n",
    "            and not 'sqft'   in c \\\n",
    "            and not 'cnt'    in c \\\n",
    "            and not 'nbr'    in c \\\n",
    "            and not 'number' in c:\n",
    "            cat_feature_inds.append(i)\n",
    "\n",
    "    print(\"Categorical features:\")\n",
    "    print([data.columns[ind] for ind in cat_feature_inds])\n",
    "    print('........')\n",
    "    \n",
    "    return cat_feature_inds\n",
    "\n",
    "\n",
    "def complex_features(data):\n",
    "    # Gets counts, label encoding and frequency estimates.\n",
    "    \n",
    "    # Frequency of occurances | length of codes | check if * is present\n",
    "    data['propertyzoningdesc_frq'] = get_frequency(data['propertyzoningdesc'])\n",
    "    data['propertyzoningdesc_len'] = data['propertyzoningdesc'].apply(lambda x: len(str(x)) if pd.notnull(x) else x)\n",
    "    #transactions_shuffled['propertyzoningdesc_str'] = transactions_shuffled['propertyzoningdesc'].apply(lambda x: (1 if '*' in str(x) else 0) if pd.notnull(x) else x)\n",
    "\n",
    "    # Label encoding | length of code\n",
    "    #transactions_shuffled['propertycountylandusecode_enc'] = transactions_shuffled[['propertycountylandusecode']].astype(str).apply(LabelEncoder().fit_transform)\n",
    "    #transactions_shuffled['propertycountylandusecode_len'] = transactions_shuffled['propertycountylandusecode'].apply(lambda x: x if pd.isnull(x) else len(x))\n",
    "\n",
    "    # Zip code area extraction\n",
    "    data['regionidzip_ab']  = data['regionidzip'].apply(lambda x: x if pd.isnull(x) else str(x)[:2]).astype(float)\n",
    "    data['regionidzip_abc'] = data['regionidzip'].apply(lambda x: x if pd.isnull(x) else str(x)[:3]).astype(float)\n",
    "\n",
    "    # Region neighbourhood area extraction\n",
    "    data['regionidneighborhood_ab'] = data['regionidneighborhood'].apply(lambda x: str(x)[:2] if pd.notnull(x) else x).astype(float)\n",
    "\n",
    "    # Rawcensustractandblock transformed\n",
    "    data['code_fips_cnt']  = get_frequency(data['rawcensustractandblock'].apply(lambda x: str(x)[:4]))\n",
    "    data['code_tract_cnt'] = get_frequency(data['rawcensustractandblock'].apply(lambda x: str(x)[4:11]))\n",
    "    data['code_block_cnt'] = get_frequency(data['rawcensustractandblock'].apply(lambda x: str(x)[11:]))\n",
    "    data.drop('rawcensustractandblock', axis=1, inplace=True)\n",
    "    \n",
    "    # Encode string values\n",
    "    data[['propertycountylandusecode', 'propertyzoningdesc']] = data[['propertycountylandusecode', 'propertyzoningdesc']].astype(str).apply(LabelEncoder().fit_transform)\n",
    "    \n",
    "    print('Generating complex features')\n",
    "    print('........')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Properties ...\n",
      "Loading Train ...\n",
      "Loading Sample ...\n",
      "Merge Train with Properties ...\n",
      "Added time data\n",
      "........\n",
      "Generating complex features\n",
      "........\n",
      "Added time data\n",
      "........\n",
      "Generating complex features\n",
      "........\n",
      "Tax Features 2017  ...\n",
      "Concat Train 2016 & 2017 ...\n",
      "Remove missing data fields ...\n",
      "We exclude: 13\n",
      "Remove features with one unique value !!\n",
      "We exclude: 9\n",
      "Define training features !!\n",
      "We use these for training: 50\n",
      "Define categorial features !!\n",
      "Cat features are: ['airconditioningtypeid', 'buildingqualitytypeid', 'fips', 'heatingorsystemtypeid', 'propertycountylandusecode', 'propertylandusetypeid', 'regionidcity', 'regionidcounty', 'regionidneighborhood', 'regionidzip', 'yearbuilt', 'assessmentyear', 'taxdelinquencyyear', 'day_of_week', 'month_of_year', 'quarter', 'is_weekend', 'propertyzoningdesc_frq', 'propertyzoningdesc_len', 'regionidzip_ab', 'regionidzip_abc', 'regionidneighborhood_ab']\n",
      "Replacing NaN values by -999 !!\n",
      "Training time !!\n",
      "(167888, 50) (167888,)\n",
      "Generating complex features\n",
      "........\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from catboost import CatBoostRegressor\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import datetime as dt\n",
    "\n",
    "print('Loading Properties ...')\n",
    "properties2016 = pd.read_csv('../Data/properties_2016.csv', low_memory = False)\n",
    "properties2017 = pd.read_csv('../Data/properties_2017.csv', low_memory = False)\n",
    "\n",
    "print('Loading Train ...')\n",
    "train2016 = pd.read_csv('../Data/train_2016_v2.csv', parse_dates=['transactiondate'], low_memory=False)\n",
    "train2017 = pd.read_csv('../Data/train_2017.csv', parse_dates=['transactiondate'], low_memory=False)\n",
    "\n",
    "def add_date_features(df):\n",
    "    df[\"transaction_year\"] = df[\"transactiondate\"].dt.year\n",
    "    df[\"transaction_month\"] = (df[\"transactiondate\"].dt.year - 2016)*12 + df[\"transactiondate\"].dt.month\n",
    "    df[\"transaction_day\"] = df[\"transactiondate\"].dt.day\n",
    "    df[\"transaction_quarter\"] = (df[\"transactiondate\"].dt.year - 2016)*4 +df[\"transactiondate\"].dt.quarter\n",
    "    df.drop([\"transactiondate\"], inplace=True, axis=1)\n",
    "    return df\n",
    "\n",
    "#train2016 = add_date_features(train2016)\n",
    "#train2017 = add_date_features(train2017)\n",
    "\n",
    "\n",
    "print('Loading Sample ...')\n",
    "sample_submission = pd.read_csv('../Data/sample_submission.csv', low_memory = False)\n",
    "\n",
    "print('Merge Train with Properties ...')\n",
    "train2016 = pd.merge(train2016, properties2016, how = 'left', on = 'parcelid')\n",
    "train2017 = pd.merge(train2017, properties2017, how = 'left', on = 'parcelid')\n",
    "\n",
    "# Datetime transformation\n",
    "train2017 = time_data(train2017)\n",
    "train2017 = complex_features(train2017)\n",
    "\n",
    "train2016 = time_data(train2016)\n",
    "train2016 = complex_features(train2016)\n",
    "\n",
    "print('Tax Features 2017  ...')\n",
    "#train2017.iloc[:, train2017.columns.str.startswith('tax')] = np.nan\n",
    "\n",
    "print('Concat Train 2016 & 2017 ...')\n",
    "train_df = pd.concat([train2016, train2017], axis = 0)\n",
    "test_df = pd.merge(sample_submission[['ParcelId']], properties2016.rename(columns = {'parcelid': 'ParcelId'}), how = 'left', on = 'ParcelId')\n",
    "\n",
    "del properties2016, properties2017, train2016, train2017\n",
    "gc.collect();\n",
    "\n",
    "print('Remove missing data fields ...')\n",
    "\n",
    "missing_perc_thresh = 0.98\n",
    "exclude_missing = []\n",
    "num_rows = train_df.shape[0]\n",
    "for c in train_df.columns:\n",
    "    num_missing = train_df[c].isnull().sum()\n",
    "    if num_missing == 0:\n",
    "        continue\n",
    "    missing_frac = num_missing / float(num_rows)\n",
    "    if missing_frac > missing_perc_thresh:\n",
    "        exclude_missing.append(c)\n",
    "print(\"We exclude: %s\" % len(exclude_missing))\n",
    "\n",
    "del num_rows, missing_perc_thresh\n",
    "gc.collect();\n",
    "\n",
    "print (\"Remove features with one unique value !!\")\n",
    "exclude_unique = []\n",
    "for c in train_df.columns:\n",
    "    num_uniques = len(train_df[c].unique())\n",
    "    if train_df[c].isnull().sum() != 0:\n",
    "        num_uniques -= 1\n",
    "    if num_uniques == 1:\n",
    "        exclude_unique.append(c)\n",
    "print(\"We exclude: %s\" % len(exclude_unique))\n",
    "\n",
    "print (\"Define training features !!\")\n",
    "exclude_other = ['parcelid', 'logerror','propertyzoningdesc']\n",
    "train_features = []\n",
    "for c in train_df.columns:\n",
    "    if c not in exclude_missing \\\n",
    "       and c not in exclude_other and c not in exclude_unique:\n",
    "        train_features.append(c)\n",
    "print(\"We use these for training: %s\" % len(train_features))\n",
    "\n",
    "print (\"Define categorial features !!\")\n",
    "cat_feature_inds = []\n",
    "cat_unique_thresh = 1000\n",
    "for i, c in enumerate(train_features):\n",
    "    num_uniques = len(train_df[c].unique())\n",
    "    if num_uniques < cat_unique_thresh \\\n",
    "        and not 'sqft' in c \\\n",
    "        and not 'cnt' in c \\\n",
    "        and not 'nbr' in c \\\n",
    "        and not 'number' in c:\n",
    "        cat_feature_inds.append(i)\n",
    "        \n",
    "print(\"Cat features are: %s\" % [train_features[ind] for ind in cat_feature_inds])\n",
    "\n",
    "print (\"Replacing NaN values by -999 !!\")\n",
    "train_df.fillna(-999, inplace=True)\n",
    "test_df.fillna(-999, inplace=True)\n",
    "\n",
    "print (\"Training time !!\")\n",
    "X_train = train_df[train_features]\n",
    "y_train = train_df.logerror\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'ParcelId': test_df['ParcelId'],\n",
    "})\n",
    "test_dates = {\n",
    "    '201610': pd.Timestamp('2016-09-30'),\n",
    "    '201611': pd.Timestamp('2016-10-31'),\n",
    "    '201612': pd.Timestamp('2016-11-30'),\n",
    "    '201710': pd.Timestamp('2017-09-30'),\n",
    "    '201711': pd.Timestamp('2017-10-31'),\n",
    "    '201712': pd.Timestamp('2017-11-30')\n",
    "}\n",
    "\n",
    "test_df = complex_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 0\n",
      "Predicting for: 201610 ... \n",
      "Added time data\n",
      "........\n",
      "Predicting for: 201611 ... \n",
      "Added time data\n",
      "........\n",
      "Predicting for: 201612 ... \n",
      "Added time data\n",
      "........\n",
      "Predicting for: 201710 ... \n",
      "Added time data\n",
      "........\n",
      "Predicting for: 201711 ... \n",
      "Added time data\n",
      "........\n",
      "Predicting for: 201712 ... \n",
      "Added time data\n",
      "........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [20:59<1:23:57, 1259.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1\n",
      "Predicting for: 201610 ... \n",
      "Added time data\n",
      "........\n",
      "Predicting for: 201611 ... \n",
      "Added time data\n",
      "........\n",
      "Predicting for: 201612 ... \n",
      "Added time data\n",
      "........\n",
      "Predicting for: 201710 ... \n",
      "Added time data\n",
      "........\n",
      "Predicting for: 201711 ... \n",
      "Added time data\n",
      "........\n",
      "Predicting for: 201712 ... \n",
      "Added time data\n",
      "........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [41:13<1:01:50, 1236.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 2\n",
      "Predicting for: 201610 ... \n",
      "Added time data\n",
      "........\n",
      "Predicting for: 201611 ... \n",
      "Added time data\n",
      "........\n",
      "Predicting for: 201612 ... \n",
      "Added time data\n",
      "........\n",
      "Predicting for: 201710 ... \n",
      "Added time data\n",
      "........\n",
      "Predicting for: 201711 ... \n",
      "Added time data\n",
      "........\n",
      "Predicting for: 201712 ... \n",
      "Added time data\n",
      "........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [1:04:21<42:54, 1287.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 3\n",
      "Predicting for: 201610 ... \n",
      "Added time data\n",
      "........\n",
      "Predicting for: 201611 ... \n",
      "Added time data\n",
      "........\n",
      "Predicting for: 201612 ... \n",
      "Added time data\n",
      "........\n",
      "Predicting for: 201710 ... \n",
      "Added time data\n",
      "........\n",
      "Predicting for: 201711 ... \n",
      "Added time data\n",
      "........\n",
      "Predicting for: 201712 ... \n",
      "Added time data\n",
      "........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [1:25:22<21:20, 1280.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 4\n",
      "Predicting for: 201610 ... \n",
      "Added time data\n",
      "........\n",
      "Predicting for: 201611 ... \n",
      "Added time data\n",
      "........\n",
      "Predicting for: 201612 ... \n",
      "Added time data\n",
      "........\n",
      "Predicting for: 201710 ... \n",
      "Added time data\n",
      "........\n",
      "Predicting for: 201711 ... \n",
      "Added time data\n",
      "........\n",
      "Predicting for: 201712 ... \n",
      "Added time data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 5/5 [1:45:54<00:00, 1270.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_ensembles = 5\n",
    "for label, test_date in test_dates.items():\n",
    "    submission[label] = 0\n",
    "\n",
    "for i in tqdm(range(num_ensembles)):\n",
    "    print('Training model', i)\n",
    "    model = CatBoostRegressor(\n",
    "        iterations=630, learning_rate=0.03,\n",
    "        depth=6, l2_leaf_reg=3,\n",
    "        loss_function='MAE',\n",
    "        eval_metric='MAE',\n",
    "        random_seed=i)\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        cat_features=cat_feature_inds)\n",
    "\n",
    "    for label, test_date in test_dates.items():\n",
    "        print(\"Predicting for: %s ... \" % (label))\n",
    "        test_df['transactiondate'] = test_date \n",
    "        test_df = time_data(test_df)\n",
    "        X_test = test_df[train_features]\n",
    "\n",
    "        submission[label] += model.predict(X_test)\n",
    "    \n",
    "#submission.to_csv('catboost_sample.csv', float_format='%.6f',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv('catboost_sample.csv', float_format='%.6f',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(submission[list(test_dates.keys())]/5).to_csv('catboost_sample.csv', float_format='%.6f',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new = pd.DataFrame()\n",
    "new['ParcelId'] = submission['ParcelId']\n",
    "new[list(test_dates.keys())] = (submission[list(test_dates.keys())]/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new.to_csv('catboost_sample.csv', float_format='%.6f',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
