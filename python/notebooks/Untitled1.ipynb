{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Properties ...\n",
      "Loading Train ...\n",
      "Loading Sample ...\n",
      "Merge Train with Properties ...\n",
      "Tax Features 2017  ...\n",
      "Concat Train 2016 & 2017 ...\n",
      "Remove missing data fields ...\n",
      "We exclude: 14\n",
      "Remove features with one unique value !!\n",
      "We exclude: 11\n",
      "Define training features !!\n",
      "We use these for training: 41\n",
      "Define categorial features !!\n",
      "Cat features are: ['transaction_month', 'transaction_day', 'transaction_quarter', 'airconditioningtypeid', 'buildingqualitytypeid', 'fips', 'heatingorsystemtypeid', 'propertycountylandusecode', 'propertylandusetypeid', 'regionidcity', 'regionidcounty', 'regionidneighborhood', 'regionidzip', 'yearbuilt', 'taxdelinquencyyear']\n",
      "Replacing NaN values by -999 !!\n",
      "Training time !!\n",
      "(77613, 41) (77613,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2985217, 41)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [20:43<00:00, 248.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for: 201610 ... \n",
      "Predicting for: 201611 ... \n",
      "Predicting for: 201612 ... \n",
      "Predicting for: 201710 ... \n",
      "Predicting for: 201711 ... \n",
      "Predicting for: 201712 ... \n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from catboost import CatBoostRegressor\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import datetime as dt\n",
    "\n",
    "print('Loading Properties ...')\n",
    "properties2016 = pd.read_csv('../Data/properties_2016.csv', low_memory = False)\n",
    "properties2017 = pd.read_csv('../Data/properties_2017.csv', low_memory = False)\n",
    "\n",
    "print('Loading Train ...')\n",
    "train2016 = pd.read_csv('../Data/train_2016_v2.csv', parse_dates=['transactiondate'], low_memory=False)\n",
    "train2017 = pd.read_csv('../Data/train_2017.csv', parse_dates=['transactiondate'], low_memory=False)\n",
    "\n",
    "def add_date_features(df):\n",
    "    df[\"transaction_year\"] = df[\"transactiondate\"].dt.year\n",
    "    df[\"transaction_month\"] = (df[\"transactiondate\"].dt.year - 2016)*12 + df[\"transactiondate\"].dt.month\n",
    "    df[\"transaction_day\"] = df[\"transactiondate\"].dt.day\n",
    "    df[\"transaction_quarter\"] = (df[\"transactiondate\"].dt.year - 2016)*4 +df[\"transactiondate\"].dt.quarter\n",
    "    df.drop([\"transactiondate\"], inplace=True, axis=1)\n",
    "    return df\n",
    "\n",
    "train2016 = add_date_features(train2016)\n",
    "train2017 = add_date_features(train2017)\n",
    "\n",
    "print('Loading Sample ...')\n",
    "sample_submission = pd.read_csv('../Data/sample_submission.csv', low_memory = False)\n",
    "\n",
    "print('Merge Train with Properties ...')\n",
    "train2016 = pd.merge(train2016, properties2016, how = 'left', on = 'parcelid')\n",
    "train2017 = pd.merge(train2017, properties2017, how = 'left', on = 'parcelid')\n",
    "\n",
    "print('Tax Features 2017  ...')\n",
    "#train2017.iloc[:, train2017.columns.str.startswith('tax')] = np.nan\n",
    "\n",
    "print('Concat Train 2016 & 2017 ...')\n",
    "train_df = train2017.copy()#pd.concat([train2016, train2017], axis = 0)\n",
    "test_df = pd.merge(sample_submission[['ParcelId']], properties2016.rename(columns = {'parcelid': 'ParcelId'}), how = 'left', on = 'ParcelId')\n",
    "\n",
    "del properties2016, properties2017, train2016, train2017\n",
    "gc.collect();\n",
    "\n",
    "print('Remove missing data fields ...')\n",
    "\n",
    "missing_perc_thresh = 0.98\n",
    "exclude_missing = []\n",
    "num_rows = train_df.shape[0]\n",
    "for c in train_df.columns:\n",
    "    num_missing = train_df[c].isnull().sum()\n",
    "    if num_missing == 0:\n",
    "        continue\n",
    "    missing_frac = num_missing / float(num_rows)\n",
    "    if missing_frac > missing_perc_thresh:\n",
    "        exclude_missing.append(c)\n",
    "print(\"We exclude: %s\" % len(exclude_missing))\n",
    "\n",
    "del num_rows, missing_perc_thresh\n",
    "gc.collect();\n",
    "\n",
    "print (\"Remove features with one unique value !!\")\n",
    "exclude_unique = []\n",
    "for c in train_df.columns:\n",
    "    num_uniques = len(train_df[c].unique())\n",
    "    if train_df[c].isnull().sum() != 0:\n",
    "        num_uniques -= 1\n",
    "    if num_uniques == 1:\n",
    "        exclude_unique.append(c)\n",
    "print(\"We exclude: %s\" % len(exclude_unique))\n",
    "\n",
    "print (\"Define training features !!\")\n",
    "exclude_other = ['parcelid', 'logerror','propertyzoningdesc']\n",
    "train_features = []\n",
    "for c in train_df.columns:\n",
    "    if c not in exclude_missing \\\n",
    "       and c not in exclude_other and c not in exclude_unique:\n",
    "        train_features.append(c)\n",
    "print(\"We use these for training: %s\" % len(train_features))\n",
    "\n",
    "print (\"Define categorial features !!\")\n",
    "cat_feature_inds = []\n",
    "cat_unique_thresh = 1000\n",
    "for i, c in enumerate(train_features):\n",
    "    num_uniques = len(train_df[c].unique())\n",
    "    if num_uniques < cat_unique_thresh \\\n",
    "        and not 'sqft' in c \\\n",
    "        and not 'cnt' in c \\\n",
    "        and not 'nbr' in c \\\n",
    "        and not 'number' in c:\n",
    "        cat_feature_inds.append(i)\n",
    "        \n",
    "print(\"Cat features are: %s\" % [train_features[ind] for ind in cat_feature_inds])\n",
    "\n",
    "print (\"Replacing NaN values by -999 !!\")\n",
    "train_df.fillna(-999, inplace=True)\n",
    "test_df.fillna(-999, inplace=True)\n",
    "\n",
    "print (\"Training time !!\")\n",
    "X_train = train_df[train_features]\n",
    "y_train = train_df.logerror\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "test_df['transactiondate'] = pd.Timestamp('2016-12-01') \n",
    "test_df = add_date_features(test_df)\n",
    "X_test = test_df[train_features]\n",
    "print(X_test.shape)\n",
    "\n",
    "num_ensembles = 5\n",
    "y_pred = 0.0\n",
    "for i in tqdm(range(num_ensembles)):\n",
    "    model = CatBoostRegressor(\n",
    "        iterations=630, learning_rate=0.03,\n",
    "        depth=6, l2_leaf_reg=3,\n",
    "        loss_function='MAE',\n",
    "        eval_metric='MAE',\n",
    "        random_seed=i)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        cat_features=cat_feature_inds)\n",
    "    y_pred += model.predict(X_test)\n",
    "y_pred /= num_ensembles\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'ParcelId': test_df['ParcelId'],\n",
    "})\n",
    "test_dates = {\n",
    "    '201610': pd.Timestamp('2016-09-30'),\n",
    "    '201611': pd.Timestamp('2016-10-31'),\n",
    "    '201612': pd.Timestamp('2016-11-30'),\n",
    "    '201710': pd.Timestamp('2017-09-30'),\n",
    "    '201711': pd.Timestamp('2017-10-31'),\n",
    "    '201712': pd.Timestamp('2017-11-30')\n",
    "}\n",
    "for label, test_date in test_dates.items():\n",
    "    print(\"Predicting for: %s ... \" % (label))\n",
    "    submission[label] = y_pred\n",
    "    \n",
    "submission.to_csv('Only_CatBoost.csv', float_format='%.6f',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  This is forked form Kamil Kaczmarek's Simple LGBM model (the only kernel I've found so far\n",
    "#  that makes a serious attempt to combine 2016 and 2017 data while accounting for the leak),\n",
    "#  but I don't think Kamil handles the leak optimally.\n",
    "\n",
    "#  1. We should use 2017 properties data for all non-tax variables in the 2016 train set.\n",
    "#     (It's more complete, and there is no known leakage outside the tax variables.)\n",
    "#  2. We shoold use 2016 properties data for the tax variables in the 2016 train set.\n",
    "#  3. We should use 2017 properties data for all variables in the 2017 train set.\n",
    "#  4. Predictions for 2016 should use the same properties data as the 2016 train set.\n",
    "#     (Otherwise they would be using tax infomration not available in 2016.)\n",
    "#  5. Predictions for 2017 should use the same properties data as the 2017 train set.\n",
    "\n",
    "# In Kamil's version, he doesn't use the 2017 tax variables for training at all.\n",
    "# (They are set to NA in the 2017 train data and not merged with 2017 train data.)\n",
    "# That procedure does avoid the leak, \n",
    "# but I think it also throws away potentially useful information,\n",
    "# so in this verison I've tried to use all the information we can without leaking.\n",
    "\n",
    "# (Note:  I'm not entirely comfortable with using future data to predict the past,\n",
    "#   even when there isn't an identified leak, but even Kamil's version does that,\n",
    "#   since it trains on the full training set that includes sales from 2017.\n",
    "#   It's probably OK here, since we're only using 2016 as a sanity check.)\n",
    "\n",
    "\n",
    "# Anyhow, there is a significant chance that I got the logic wrong or introduced bugs,\n",
    "# so if anyone thinks this is wrong, please let me know.\n",
    "\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "\n",
    "print('Loading data...')\n",
    "# Load raw data\n",
    "properties2016_raw = pd.read_csv('../input/properties_2016.csv', low_memory = False)\n",
    "properties2017 = pd.read_csv('../input/properties_2017.csv', low_memory = False)\n",
    "train2016 = pd.read_csv('../input/train_2016_v2.csv')\n",
    "train2017 = pd.read_csv('../input/train_2017.csv')\n",
    "sample_submission = pd.read_csv('../input/sample_submission.csv', low_memory = False)\n",
    "\n",
    "# Create a new version of 2016 properties data that takes all non-tax variables from 2017\n",
    "taxvars = ['structuretaxvaluedollarcnt', 'landtaxvaluedollarcnt', 'taxvaluedollarcnt', 'taxamount']\n",
    "tax2016 = properties2016_raw[['parcelid']+taxvars]\n",
    "properties2016 = properties2017.drop(taxvars,axis=1).merge(tax2016, \n",
    "                 how='left', on='parcelid').reindex_axis(properties2017.columns, axis=1)\n",
    "\n",
    "# Create a training data set\n",
    "train2016 = pd.merge(train2016, properties2016, how = 'left', on = 'parcelid')\n",
    "train2017 = pd.merge(train2017, properties2017, how = 'left', on = 'parcelid')\n",
    "train = pd.concat([train2016, train2017], axis = 0)\n",
    "\n",
    "# Create separate test data sets for 2016 and 2017\n",
    "test2016 = pd.merge(sample_submission[['ParcelId']], properties2016.rename(columns = {'parcelid': 'ParcelId'}), \n",
    "                how = 'left', on = 'ParcelId')\n",
    "test2017 = pd.merge(sample_submission[['ParcelId']], properties2017.rename(columns = {'parcelid': 'ParcelId'}), \n",
    "                how = 'left', on = 'ParcelId')\n",
    "del properties2016, properties2017, train2016, train2017\n",
    "gc.collect();\n",
    "\n",
    "\n",
    "print('Memory usage reduction...')\n",
    "\n",
    "\n",
    "train[['latitude', 'longitude']] /= 1e6\n",
    "train['censustractandblock'] /= 1e12\n",
    "\n",
    "def preptest(test):\n",
    "    test[['latitude', 'longitude']] /= 1e6\n",
    "    test[['latitude', 'longitude']] /= 1e6\n",
    "    test['censustractandblock'] /= 1e12\n",
    "    test['censustractandblock'] /= 1e12\n",
    "\n",
    "    for column in test.columns:\n",
    "        if test[column].dtype == int:\n",
    "            test[column] = test[column].astype(np.int32)\n",
    "        if test[column].dtype == float:\n",
    "            test[column] = test[column].astype(np.float32)\n",
    "\n",
    "preptest(test2016)\n",
    "preptest(test2017)\n",
    "        \n",
    "print('Feature engineering...')\n",
    "train['month'] = (pd.to_datetime(train['transactiondate']).dt.year - 2016)*12 + pd.to_datetime(train['transactiondate']).dt.month\n",
    "train = train.drop('transactiondate', axis = 1)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "non_number_columns = train.dtypes[train.dtypes == object].index.values\n",
    "\n",
    "for column in non_number_columns:\n",
    "    train_test = pd.concat([train[column], test2016[column], test2017[column]], axis = 0)\n",
    "    encoder = LabelEncoder().fit(train_test.astype(str))\n",
    "    train[column] = encoder.transform(train[column].astype(str)).astype(np.int32)\n",
    "    test2016[column] = encoder.transform(test2016[column].astype(str)).astype(np.int32)\n",
    "    test2017[column] = encoder.transform(test2017[column].astype(str)).astype(np.int32)\n",
    "    \n",
    "feature_names = [feature for feature in train.columns[2:] if feature != 'month']\n",
    "\n",
    "month_avgs = train.groupby('month').agg('mean')['logerror'].values - train['logerror'].mean()\n",
    "                             \n",
    "print('Preparing arrays and throwing out outliers...')\n",
    "X_train = train[feature_names].values\n",
    "y_train = train['logerror'].values\n",
    "X_test2016 = test2016[feature_names].values\n",
    "X_test2017 = test2017[feature_names].values\n",
    "\n",
    "del test2016, test2017;\n",
    "gc.collect();\n",
    "\n",
    "month_values = train['month'].values\n",
    "month_avg_values = np.array([month_avgs[month - 1] for month in month_values]).reshape(-1, 1)\n",
    "X_train = np.hstack([X_train, month_avg_values])\n",
    "\n",
    "X_train = X_train[np.abs(y_train) < 0.4, :]\n",
    "y_train = y_train[np.abs(y_train) < 0.4]\n",
    "\n",
    "\n",
    "print('Training LGBM model...')\n",
    "ltrain = lgb.Dataset(X_train, label = y_train)\n",
    "\n",
    "params = {}\n",
    "params['metric'] = 'mae'\n",
    "params['max_depth'] = 100\n",
    "params['num_leaves'] = 32\n",
    "params['feature_fraction'] = .85\n",
    "params['bagging_fraction'] = .95\n",
    "params['bagging_freq'] = 8\n",
    "params['learning_rate'] = 0.0025\n",
    "params['verbosity'] = 0\n",
    "\n",
    "lgb_model = lgb.train(params, ltrain, valid_sets = [ltrain], verbose_eval=200, num_boost_round=2930)\n",
    "                  \n",
    "                  \n",
    "print('Making predictions and praying for good results...')\n",
    "X_test2016 = np.hstack([X_test2016, np.zeros((X_test2016.shape[0], 1))])\n",
    "X_test2017 = np.hstack([X_test2016, np.zeros((X_test2017.shape[0], 1))])\n",
    "folds = 20\n",
    "n = int(X_test2016.shape[0] / folds)\n",
    "\n",
    "for j in range(folds):\n",
    "    results = pd.DataFrame()\n",
    "\n",
    "    if j < folds - 1:\n",
    "            X_test2016_ = X_test2016[j*n: (j+1)*n, :]\n",
    "            X_test2017_ = X_test2017[j*n: (j+1)*n, :]\n",
    "            results['ParcelId'] = sample_submission['ParcelId'].iloc[j*n: (j+1)*n]\n",
    "    else:\n",
    "            X_test2016_ = X_test2016[j*n: , :]\n",
    "            X_test2017_ = X_test2017[j*n: , :]\n",
    "            results['ParcelId'] = sample_submission['ParcelId'].iloc[j*n: ]\n",
    "\n",
    "    for month in [10, 11, 12]:\n",
    "        X_test2016_[:, -1] = month_avgs[month - 1]\n",
    "        assert X_test2016_.shape[1] == X_test2016.shape[1]\n",
    "        y_pred = lgb_model.predict(X_test2016_)\n",
    "        results['2016'+ str(month)] = y_pred\n",
    "        \n",
    "    X_test2017_[:, -1] = month_avgs[20]\n",
    "    assert X_test2017_.shape[1] == X_test2017.shape[1]\n",
    "    y_pred = lgb_model.predict(X_test2017_)\n",
    "    results['201710'] = y_pred\n",
    "    results['201711'] = y_pred\n",
    "    results['201712'] = y_pred\n",
    "    \n",
    "    if j == 0:\n",
    "        results_ = results.copy()\n",
    "    else:\n",
    "        results_ = pd.concat([results_, results], axis = 0)\n",
    "    print('{}% completed'.format(round(100*(j+1)/folds)))\n",
    "    \n",
    "    \n",
    "print('Saving predictions...')\n",
    "results = results_[sample_submission.columns]\n",
    "assert results.shape == sample_submission.shape\n",
    "results.to_csv('submission.csv', index = False, float_format = '%.5f')\n",
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
