{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$('<div id=\"toc\"></div>').css({position: 'fixed', top: '120px', left: 0}).appendTo(document.body);\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import gc \n",
    "\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style()\n",
    "\n",
    "import xgboost as xg\n",
    "from xgboost import XGBModel\n",
    "from xgboost import plot_importance\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit, ShuffleSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import cross_validation\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autotime\n",
    "%load_ext line_profiler\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Kaggle Kernel Data Preparation & Own Implementations\n",
    "\n",
    "def plot_data(test, pred, sample, title, width=40, height=10, linewidth=0.5, color1='white', color2='orange'):\n",
    "    \"\"\" Plotting method. \"\"\"\n",
    "    fig = plt.figure(figsize=(width, height))\n",
    "    plt.plot(pred[:sample], color=color1, zorder=4, linewidth=linewidth, label='%s Prediction'%(title))\n",
    "    plt.plot(test[:sample], color=color2, zorder=3, linewidth=linewidth, label='%s True Data'%(title))\n",
    "    plt.title = title\n",
    "    plt.legend()\n",
    "\n",
    "# Frequency count\n",
    "def get_frequency(data):\n",
    "    # Gets the frequency of a column's values in 'data'. Pass on a series.\n",
    "    vals = pd.merge(data.to_frame(), data.value_counts().reset_index(), \n",
    "                    how='left', left_on=data.to_frame().columns[0], right_on='index').iloc[:, -1:].values\n",
    "    return vals\n",
    "  \n",
    "def time_data(data):\n",
    "    data['transactiondate'] = pd.to_datetime(data['transactiondate'])\n",
    "    data['day_of_week']     = data['transactiondate'].dt.dayofweek\n",
    "    data['month_of_year']   = data['transactiondate'].dt.month\n",
    "    data['quarter']         = data['transactiondate'].dt.quarter\n",
    "    data['is_weekend']      = (data['day_of_week'] < 5).astype(int)\n",
    "    data.drop('transactiondate', axis=1, inplace=True)\n",
    "    \n",
    "    print('Added time data')\n",
    "    print('........')\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def column_excluder(data, missing_perc_thresh=0.98):\n",
    "    # Quick clean from https://www.kaggle.com/seesee/concise-catboost-starter-ensemble-plb-0-06435\n",
    "    \n",
    "    exclude_missing = []\n",
    "    exclude_unique = []\n",
    "    num_rows = data.shape[0]\n",
    "    for c in data.columns:\n",
    "        num_missing = data[c].isnull().sum()\n",
    "        if num_missing == 0:\n",
    "            continue\n",
    "        missing_frac = num_missing / float(num_rows)\n",
    "        if missing_frac > missing_perc_thresh:\n",
    "            exclude_missing.append(c)\n",
    "\n",
    "        num_uniques = len(data[c].unique())\n",
    "        if data[c].isnull().sum() != 0:\n",
    "            num_uniques -= 1\n",
    "        if num_uniques == 1:\n",
    "            exclude_unique.append(c)\n",
    "            \n",
    "    to_exclude = list(set(exclude_missing + exclude_unique))\n",
    "    \n",
    "    print('Excluded columns:')\n",
    "    print(to_exclude)\n",
    "    print('........')\n",
    "    \n",
    "    return to_exclude\n",
    "\n",
    "def categorical_features(data):\n",
    "    # Quick categories from https://www.kaggle.com/seesee/concise-catboost-starter-ensemble-plb-0-06435\n",
    "        \n",
    "    cat_feature_inds = []\n",
    "    cat_unique_thresh = 1000\n",
    "    for i, c in enumerate(data.columns):\n",
    "        num_uniques = len(data[c].unique())\n",
    "        if num_uniques < cat_unique_thresh \\\n",
    "            and not 'sqft'   in c \\\n",
    "            and not 'cnt'    in c \\\n",
    "            and not 'nbr'    in c \\\n",
    "            and not 'number' in c:\n",
    "            cat_feature_inds.append(i)\n",
    "\n",
    "    print(\"Categorical features:\")\n",
    "    print([data.columns[ind] for ind in cat_feature_inds])\n",
    "    print('........')\n",
    "    \n",
    "    return cat_feature_inds\n",
    "\n",
    "\n",
    "def complex_features(data):\n",
    "    # Gets counts, label encoding and frequency estimates.\n",
    "    \n",
    "    # Frequency of occurances | length of codes | check if * is present\n",
    "    data['propertyzoningdesc_frq'] = get_frequency(data['propertyzoningdesc'])\n",
    "    data['propertyzoningdesc_len'] = data['propertyzoningdesc'].apply(lambda x: len(x) if pd.notnull(x) else x)\n",
    "    #transactions_shuffled['propertyzoningdesc_str'] = transactions_shuffled['propertyzoningdesc'].apply(lambda x: (1 if '*' in str(x) else 0) if pd.notnull(x) else x)\n",
    "\n",
    "    # Label encoding | length of code\n",
    "    #transactions_shuffled['propertycountylandusecode_enc'] = transactions_shuffled[['propertycountylandusecode']].astype(str).apply(LabelEncoder().fit_transform)\n",
    "    #transactions_shuffled['propertycountylandusecode_len'] = transactions_shuffled['propertycountylandusecode'].apply(lambda x: x if pd.isnull(x) else len(x))\n",
    "\n",
    "    # Zip code area extraction\n",
    "    data['regionidzip_ab']  = data['regionidzip'].apply(lambda x: x if pd.isnull(x) else str(x)[:2]).astype(float)\n",
    "    data['regionidzip_abc'] = data['regionidzip'].apply(lambda x: x if pd.isnull(x) else str(x)[:3]).astype(float)\n",
    "\n",
    "    # Region neighbourhood area extraction\n",
    "    data['regionidneighborhood_ab'] = data['regionidneighborhood'].apply(lambda x: str(x)[:2] if pd.notnull(x) else x).astype(float)\n",
    "\n",
    "    # Rawcensustractandblock transformed\n",
    "    data['code_fips_cnt']  = get_frequency(data['rawcensustractandblock'].apply(lambda x: str(x)[:4]))\n",
    "    data['code_tract_cnt'] = get_frequency(data['rawcensustractandblock'].apply(lambda x: str(x)[4:11]))\n",
    "    data['code_block_cnt'] = get_frequency(data['rawcensustractandblock'].apply(lambda x: str(x)[11:]))\n",
    "    data.drop('rawcensustractandblock', axis=1, inplace=True)\n",
    "    \n",
    "    # Encode string values\n",
    "    data[['propertycountylandusecode', 'propertyzoningdesc']] = data[['propertycountylandusecode', 'propertyzoningdesc']].astype(str).apply(LabelEncoder().fit_transform)\n",
    "    \n",
    "    print('Generating complex features')\n",
    "    print('........')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Kaggle Kernel Data Preparation\n",
    "\n",
    "seed = 11\n",
    "np.random.seed(seed)\n",
    "drop_tax = False\n",
    "\n",
    "train2016 = pd.read_csv(\"../Data/train_2016_v2.csv\", parse_dates=[\"transactiondate\"], low_memory=False)\n",
    "train2017 = pd.read_csv('../Data/train_2017.csv', parse_dates=['transactiondate'], low_memory=False)\n",
    "\n",
    "if drop_tax:\n",
    "    # Avoids external bias\n",
    "    print('Removing tax features from 2017')\n",
    "    train2017.iloc[:, train2017.columns.str.startswith('tax')] = np.nan\n",
    "\n",
    "properties2016 = pd.read_csv('../Data/properties_2016.csv', low_memory = False)\n",
    "properties2017 = pd.read_csv('../Data/properties_2017.csv', low_memory = False)\n",
    "\n",
    "sample = pd.read_csv('../Data/sample_submission.csv')\n",
    "\n",
    "transactions2016 = pd.merge(train2016, properties2016, how='left', on=['parcelid']).sample(frac=1)\n",
    "transactions2017 = pd.merge(train2017, properties2017, how='left', on=['parcelid']).sample(frac=1)\n",
    "transactions = pd.concat([transactions2016, transactions2017], axis = 0)\n",
    "\n",
    "#transactions[['propertycountylandusecode', 'propertyzoningdesc']] = transactions[['propertycountylandusecode', 'propertyzoningdesc']].astype(str).apply(LabelEncoder().fit_transform)\n",
    "transactions['taxdelinquencyflag'].replace('Y', 1, inplace=True)\n",
    "    \n",
    "# Clean columns\n",
    "to_drop = column_excluder(transactions)\n",
    "transactions.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "# Time data\n",
    "transactions = time_data(transactions)\n",
    "transactions = complex_features(transactions)\n",
    "\n",
    "x_all = transactions.drop(['parcelid', 'propertyzoningdesc', 'propertycountylandusecode', 'fireplacecnt'], axis=1)\n",
    "y_all = transactions['logerror']\n",
    "#x_all.drop(['hashottuborspa' 'taxdelinquencyflag' 'fireplaceflag'], axis=1)\n",
    "#x_all['hashottuborspa'].astype(float, inplace=True)\n",
    "\n",
    "#x_all.fillna(-1, inplace=True)#.astype(str)#.apply(LabelEncoder().fit_transform)\n",
    "\n",
    "x_all.fillna(x_all.median(),inplace = True)\n",
    "\n",
    "ratio = 0.0\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_all, y_all, test_size=ratio)\n",
    "\n",
    "x_train_label = x_train['logerror'].copy()\n",
    "x_train_data = x_train.drop(['logerror'], axis=1).copy()\n",
    "\n",
    "# Drop outliers \n",
    "x_train = x_train[(x_train['logerror'] > -0.4) & (x_train['logerror'] < 0.419)]\n",
    "y_train = x_train['logerror']\n",
    "x_train.drop('logerror', axis=1, inplace=True)\n",
    "x_valid.drop('logerror', axis=1, inplace=True)\n",
    "\n",
    "cat_index = categorical_features(x_train)\n",
    "best_columns = x_train.columns\n",
    "\n",
    "y_mean = np.mean(y_train)\n",
    "\n",
    "del x_all, y_all, transactions, transactions2016, transactions2017, properties2017, properties2016, train2016, train2017\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_number_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle Kernel Data Preparation\n",
    "\n",
    "print('Loading data...')\n",
    "# Load raw data\n",
    "properties2016_raw = pd.read_csv('../Data/properties_2016.csv', low_memory = False)\n",
    "properties2017 = pd.read_csv('../Data/properties_2017.csv', low_memory = False)\n",
    "train2016 = pd.read_csv('../Data/train_2016_v2.csv')\n",
    "train2017 = pd.read_csv('../Data/train_2017.csv')\n",
    "sample_submission = pd.read_csv('../Data/sample_submission.csv', low_memory = False)\n",
    "\n",
    "# Create a new version of 2016 properties data that takes all non-tax variables from 2017\n",
    "taxvars = ['structuretaxvaluedollarcnt', 'landtaxvaluedollarcnt', 'taxvaluedollarcnt', 'taxamount']\n",
    "tax2016 = properties2016_raw[['parcelid']+taxvars]\n",
    "properties2016 = properties2017.drop(taxvars,axis=1).merge(tax2016, \n",
    "                 how='left', on='parcelid').reindex_axis(properties2017.columns, axis=1)\n",
    "\n",
    "# Create a training data set\n",
    "train2016 = pd.merge(train2016, properties2016, how = 'left', on = 'parcelid')\n",
    "train2017 = pd.merge(train2017, properties2017, how = 'left', on = 'parcelid')\n",
    "train = pd.concat([train2016, train2017], axis = 0)\n",
    "\n",
    "# Create separate test data sets for 2016 and 2017\n",
    "test2016 = pd.merge(sample_submission[['ParcelId']], properties2016.rename(columns = {'parcelid': 'ParcelId'}), \n",
    "                how = 'left', on = 'ParcelId')\n",
    "test2017 = pd.merge(sample_submission[['ParcelId']], properties2017.rename(columns = {'parcelid': 'ParcelId'}), \n",
    "                how = 'left', on = 'ParcelId')\n",
    "del properties2016, properties2017, train2016, train2017\n",
    "gc.collect();\n",
    "\n",
    "\n",
    "print('Memory usage reduction...')\n",
    "\n",
    "\n",
    "train[['latitude', 'longitude']] /= 1e6\n",
    "train['censustractandblock'] /= 1e12\n",
    "\n",
    "def preptest(test):\n",
    "    test[['latitude', 'longitude']] /= 1e6\n",
    "    test[['latitude', 'longitude']] /= 1e6\n",
    "    test['censustractandblock'] /= 1e12\n",
    "    test['censustractandblock'] /= 1e12\n",
    "\n",
    "    for column in test.columns:\n",
    "        if test[column].dtype == int:\n",
    "            test[column] = test[column].astype(np.int32)\n",
    "        if test[column].dtype == float:\n",
    "            test[column] = test[column].astype(np.float32)\n",
    "\n",
    "preptest(test2016)\n",
    "preptest(test2017)\n",
    "        \n",
    "print('Feature engineering...')\n",
    "train['month'] = (pd.to_datetime(train['transactiondate']).dt.year - 2016)*12 + pd.to_datetime(train['transactiondate']).dt.month\n",
    "train = train.drop('transactiondate', axis = 1)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "non_number_columns = train.dtypes[train.dtypes == object].index.values\n",
    "\n",
    "for column in non_number_columns:\n",
    "    train_test = pd.concat([train[column], test2016[column], test2017[column]], axis = 0)\n",
    "    encoder = LabelEncoder().fit(train_test.astype(str))\n",
    "    train[column] = encoder.transform(train[column].astype(str)).astype(np.int32)\n",
    "    test2016[column] = encoder.transform(test2016[column].astype(str)).astype(np.int32)\n",
    "    test2017[column] = encoder.transform(test2017[column].astype(str)).astype(np.int32)\n",
    "    \n",
    "feature_names = [feature for feature in train.columns[2:] if feature != 'month']\n",
    "\n",
    "month_avgs = train.groupby('month').agg('mean')['logerror'].values - train['logerror'].mean()\n",
    "\n",
    "X_train_all = train[feature_names].fillna(-1)\n",
    "y_train_all = train['logerror'].fillna(-1)\n",
    "\n",
    "train = train[np.abs(train['logerror']) < 0.4]\n",
    "\n",
    "print('Preparing arrays and throwing out outliers...')\n",
    "X_train = train[feature_names].fillna(-1)#.values\n",
    "y_train = train['logerror'].fillna(-1)#.values\n",
    "X_test2016 = test2016[feature_names].fillna(-1)#.values\n",
    "X_test2017 = test2017[feature_names].fillna(-1)#.values\n",
    "\n",
    "del test2016, test2017;\n",
    "gc.collect();\n",
    "\n",
    "month_values = train['month'].values\n",
    "month_avg_values = np.array([month_avgs[month - 1] for month in month_values]).reshape(-1, 1)\n",
    "X_train = np.hstack([X_train, month_avg_values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS\n",
    "model_lr = LinearRegression()\n",
    "#model_lr.fit(x_train_data, x_train_label)\n",
    "#y_pred_lr_valid = model_lr.predict(x_valid)\n",
    "#y_pred_lr_train = model_lr.predict(x_train_data)\n",
    "models['LinearRegression'] = model_lr\n",
    "\n",
    "# Make predictions on both test and validation with OLS and BR\n",
    "#predicted_mae_lr_valid = mean_absolute_error(y_valid, y_pred_lr_valid)\n",
    "#predicted_mae_lr_train = mean_absolute_error(x_train_label, y_pred_lr_train)\n",
    "\n",
    "#print('OLS MAE LR Valid:', predicted_mae_lr_valid, 'Train:', predicted_mae_lr_train)\n",
    "\n",
    "scores = cross_validation.cross_val_score(model_lr, X_train_all, y_train_all, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "print(\"%s MAE: %0.5f (+/- %0.5f)\" % (model_lr.__class__.__name__, scores.mean(), scores.std() * 2))\n",
    "\n",
    "#del y_pred_lr_valid\n",
    "#del y_pred_lr_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BayesianRidge Regression\n",
    "model_br = BayesianRidge(compute_score=True)\n",
    "#model_br.fit(x_train, y_train)\n",
    "#y_pred_br_valid = model_br.predict(x_valid)\n",
    "#y_pred_br_train = model_br.predict(x_train_data)\n",
    "models['BayesianRidge'] = model_br\n",
    "\n",
    "#predicted_mae_br_valid = mean_absolute_error(y_valid,       y_pred_br_valid)\n",
    "#predicted_mae_br_train = mean_absolute_error(x_train_label, y_pred_br_train)\n",
    "\n",
    "#print('BR MAE BayesianRidge Valid: %s \\nTrain: %s' % (predicted_mae_br_valid, predicted_mae_br_train))\n",
    "\n",
    "scores = cross_validation.cross_val_score(model_br, X_train_all, y_train_all, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "print(\"%s MAE: %0.5f (+/- %0.5f)\" % (model_br.__class__.__name__, scores.mean(), scores.std() * 2))\n",
    "\n",
    "\n",
    "#del y_pred_br_valid\n",
    "#del y_pred_br_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model_rf = RandomForestRegressor(n_jobs=1, random_state=2016, verbose=1, n_estimators=500, max_features=12)\n",
    "#model_rf.fit(x_train, y_train)\n",
    "#y_pred_rf_valid = model_rf.predict(x_valid)\n",
    "#y_pred_rf_train = model_rf.predict(x_train_data)\n",
    "models['RandomForest'] = model_rf\n",
    "\n",
    "#predicted_mae_rf_valid = mean_absolute_error(y_valid,       y_pred_rf_valid)\n",
    "#predicted_mae_rf_train = mean_absolute_error(x_train_label, y_pred_rf_train)\n",
    "\n",
    "#print('BR MAE RandomForest Valid: %s \\nTrain: %s' % (predicted_mae_rf_valid, predicted_mae_rf_train))\n",
    "\n",
    "scores = cross_validation.cross_val_score(model_rf, X_train_all, y_train_all, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "print(\"%s MAE: %0.5f (+/- %0.5f)\" % (model_rf.__class__.__name__, scores.mean(), scores.std() * 2))\n",
    "\n",
    "#del y_pred_rf_train\n",
    "#del y_pred_rf_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "model_et = ExtraTreesRegressor(\n",
    "        n_jobs=1, random_state=2016, verbose=1,\n",
    "        n_estimators=500, max_features=12)\n",
    "\n",
    "#model_et.fit(x_train, y_train)\n",
    "#y_pred_et_valid = model_et.predict(x_valid)\n",
    "#y_pred_et_train = model_et.predict(x_train_data)\n",
    "models['ExtraTrees'] = model_et\n",
    "\n",
    "#predicted_mae_et_valid = mean_absolute_error(y_valid,       y_pred_et_valid)\n",
    "#predicted_mae_et_train = mean_absolute_error(x_train_label, y_pred_et_train)\n",
    "\n",
    "#print('BR MAE ExtraTrees Valid: %s \\nTrain: %s' % (predicted_mae_et_valid, predicted_mae_et_train))\n",
    "\n",
    "#scores = cross_validation.cross_val_score(model_et, x_train, y_train, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "#print(\"%s MAE: %0.5f (+/- %0.5f)\" % (model_et.__class__.__name__, scores.mean(), scores.std() * 2))\n",
    "\n",
    "#del y_pred_et_valid\n",
    "#del y_pred_et_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "model_ab = AdaBoostRegressor()\n",
    "#model_ab.fit(x_train, y_train)\n",
    "#y_pred_ab_valid = model_ab.predict(x_valid)\n",
    "#y_pred_ab_train = model_ab.predict(x_train_data)\n",
    "models['AdaBoost'] = model_ab\n",
    "\n",
    "#predicted_mae_ab_valid = mean_absolute_error(y_valid,       y_pred_ab_valid)\n",
    "#predicted_mae_ab_train = mean_absolute_error(x_train_label, y_pred_ab_train)\n",
    "\n",
    "#print('BR MAE AdaBoost Valid: %s \\nTrain: %s' % (predicted_mae_ab_valid, predicted_mae_ab_train))\n",
    "\n",
    "#scores = cross_validation.cross_val_score(model_ab, x_train, y_train, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "#print(\"%s MAE: %0.5f (+/- %0.5f)\" % (model_ab.__class__.__name__, scores.mean(), scores.std() * 2))\n",
    "\n",
    "#del y_pred_ab_valid\n",
    "#del y_pred_ab_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_booster(x_train, y_train, x_valid, y_valid, cat_index, loss='MAE'):\n",
    "    # Cat booster train and predict\n",
    "    num_ensembles = 5\n",
    "    y_pred_valid = 0.0\n",
    "    y_pred_train = 0.0\n",
    "    \n",
    "    print('Initialising CAT Boost Regression')\n",
    "    for i in tqdm(range(num_ensembles)):\n",
    "        print('Building ensemble', i)\n",
    "        # Use CV, tune hyperparameters\n",
    "        catb = CatBoostRegressor(\n",
    "                iterations=630, learning_rate=0.03,\n",
    "                depth=6, l2_leaf_reg=3,\n",
    "                loss_function=loss,\n",
    "                eval_metric='MAE',\n",
    "                random_seed=i)\n",
    "\n",
    "        catb.fit(x_train, y_train, cat_features=cat_index)\n",
    "\n",
    "        y_pred_valid += catb.predict(x_valid)\n",
    "        y_pred_train += catb.predict(x_train)\n",
    "\n",
    "    y_pred_valid /= num_ensembles\n",
    "    y_pred_train /= num_ensembles\n",
    "\n",
    "    print('Train MAE:', mean_absolute_error(y_train, y_pred_train))\n",
    "    print('Valid MAE:', mean_absolute_error(y_valid, y_pred_valid))\n",
    "    \n",
    "    return catb, y_pred_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cb, preds = cat_booster(x_train, y_train, x_train_data, x_train_label, cat_index)\n",
    "\n",
    "print('BR MAE CatBoost Valid: %s' % (mean_absolute_error(y_valid, preds)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cb = CatBoostRegressor(\n",
    "            iterations=630, learning_rate=0.03,\n",
    "            depth=6, l2_leaf_reg=3,\n",
    "            loss_function='MAE',\n",
    "            eval_metric='MAE')\n",
    "\n",
    "models['CatBoost'] = model_cb\n",
    "\n",
    "#scores = cross_validation.cross_val_score(model_cb, x_train, y_train, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "#print(\"%s MAE: %0.5f (+/- %0.5f)\" % (model_cb.__class__.__name__, scores.mean(), scores.std() * 2))\n",
    "\n",
    "#del preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "model_gb = GradientBoostingRegressor(\n",
    "             random_state=2016, verbose=1,\n",
    "             n_estimators=500, max_features=12, max_depth=8,\n",
    "             learning_rate=0.05, subsample=0.8)\n",
    "\n",
    "#model_gb.fit(x_train, y_train)\n",
    "#y_pred_gb_valid = model_gb.predict(x_valid)\n",
    "#y_pred_gb_train = model_gb.predict(x_train_data)\n",
    "models['GradientBoosting'] = model_gb\n",
    "\n",
    "#predicted_mae_gb_valid = mean_absolute_error(y_valid,       y_pred_gb_valid)\n",
    "#predicted_mae_gb_train = mean_absolute_error(x_train_label, y_pred_gb_train)\n",
    "\n",
    "#print('BR MAE GradientBoosting Valid: %s \\nTrain: %s' % (predicted_mae_gb_valid, predicted_mae_gb_train))\n",
    "\n",
    "#scores = cross_validation.cross_val_score(model_gb, x_train, y_train, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "#print(\"%s MAE: %0.5f (+/- %0.5f)\" % (model_gb.__class__.__name__, scores.mean(), scores.std() * 2))\n",
    "\n",
    "#del y_pred_gb_valid\n",
    "#del y_pred_gb_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_xgb = {\n",
    "    'max_depth':        5,  # shuld be 0.5 to 1% of the examples\n",
    "    'subsample':        1,  # Ratio of observations to be used as samples for each tree\n",
    "    'min_child_weight': 10, # Deals with imbalanced data and prevents overfitting as the value >\n",
    "    'objective':        'reg:linear',\n",
    "    'n_estimators':     1000, # Sequential trees to be modelled.\n",
    "    'eta':              0.1,  # Shrinkage. Typically between 0.1 - 0.2 - learning rate for gradient boost (D:0.3)\n",
    "    'eval_metric':      'mae'\n",
    "}\n",
    "\n",
    "d_train = xg.DMatrix(X_train, label=y_train, missing=-1)\n",
    "#d_valid = xg.DMatrix(x_valid, label=y_valid, missing=-1)\n",
    "xgb_gs = xg.train(params_xgb, d_train, num_boost_round=250, verbose_eval=50)\n",
    "#models['XGB'] = xgb_gs\n",
    "\n",
    "#del d_train\n",
    "#del d_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def light_gbm_folds(x_train, x_valid, y_train, y_valid, params, num_ensembles):\n",
    "    # Light gbm n ensambles average predictions\n",
    "\n",
    "    y_pred_valid = 0.0\n",
    "    y_pred_train = 0.0\n",
    "    \n",
    "    d_train = lgb.Dataset(x_train, label=y_train)\n",
    "    \n",
    "    print('Initialising Light GBM')\n",
    "    for i in tqdm(range(num_ensembles)):\n",
    "        # Use CV, tune hyperparameters\n",
    "        params['seed'] = i\n",
    "        model_lgb = lgb.train(params, d_train, 430)\n",
    "        \n",
    "        lg_pred_valid = model_lgb.predict(x_valid)\n",
    "        lg_pred_train = model_lgb.predict(x_train)\n",
    "\n",
    "    lg_pred_valid /= num_ensembles\n",
    "    lg_pred_train /= num_ensembles\n",
    "    \n",
    "    print('Train MAE:', mean_absolute_error(y_train, lg_pred_train))\n",
    "    print('Valid MAE:', mean_absolute_error(y_valid, lg_pred_valid))\n",
    "    \n",
    "    return model_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import lightgbm as lgb\n",
    "\n",
    "params_lg={\n",
    "    'max_bin'          : 10,\n",
    "    'learning_rate'    : 0.0021, # shrinkage_rate\n",
    "    'boosting_type'    : 'gbdt',\n",
    "    'objective'        : 'regression',\n",
    "    'metric'           : 'mae',      \n",
    "    'sub_feature'      : 0.345 ,   \n",
    "    'bagging_fraction' : 0.85, \n",
    "    'bagging_freq'     : 40,\n",
    "    'num_leaves'       : 512,   # num_leaf\n",
    "    'min_data'         : 500,   # min_data_in_leaf\n",
    "    'min_hessian'      : 0.05,  # min_sum_hessian_in_leaf\n",
    "    'verbose'          : 1\n",
    "}\n",
    "d_train = lgb.Dataset(X_train, label=y_train)\n",
    "model_lgb = lgb.train(params_lg, d_train, 430)\n",
    "\n",
    "#model_lgb = light_gbm_folds(x_train, x_train_data, y_train, x_train_label, params_lg, num_ensembles=5)\n",
    "models['LightGBM'] = model_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.layers import Dropout, BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.noise import GaussianDropout\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "def larger_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(size, input_dim=size, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(size*2, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(size, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mae', optimizer=Adam(lr=4e-3, decay=1e-4))\n",
    "    return model\n",
    "\n",
    "# define wider model\n",
    "def wider_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(size*2, input_dim=size, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mae', optimizer=Adam(lr=4e-3, decay=1e-4))\n",
    "    return model\n",
    "\n",
    "\n",
    "# define base model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(size, input_dim=size, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mae', optimizer=Adam(lr=4e-3, decay=1e-4))\n",
    "    return model\n",
    "\n",
    "def prebuilt_nn():\n",
    "    nn = Sequential()\n",
    "    nn.add(Dense(units = 400 , kernel_initializer = 'normal', input_dim = size))\n",
    "    nn.add(PReLU())\n",
    "    nn.add(Dropout(.4))\n",
    "    nn.add(Dense(units = 160 , kernel_initializer = 'normal'))\n",
    "    nn.add(PReLU())\n",
    "    nn.add(BatchNormalization())\n",
    "    nn.add(Dropout(.6))\n",
    "    nn.add(Dense(units = 64 , kernel_initializer = 'normal'))\n",
    "    nn.add(PReLU())\n",
    "    nn.add(BatchNormalization())\n",
    "    nn.add(Dropout(.5))\n",
    "    nn.add(Dense(units = 26, kernel_initializer = 'normal'))\n",
    "    nn.add(PReLU())\n",
    "    nn.add(BatchNormalization())\n",
    "    nn.add(Dropout(.6))\n",
    "    nn.add(Dense(1, kernel_initializer='normal'))\n",
    "    nn.compile(loss='mae', optimizer=Adam(lr=4e-3, decay=1e-4))\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing\n",
    "print(\"Preprocessing neural network data...\")\n",
    "imputer= Imputer()\n",
    "imputer.fit(X_train)\n",
    "x_train_nn = imputer.transform(X_train)\n",
    "\n",
    "#imputer.fit(x_valid.iloc[:, :])\n",
    "#x_valid_nn = imputer.transform(x_valid.iloc[:, :])\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train_nn = sc.fit_transform(x_train_nn)\n",
    "#x_valid_nn = sc.transform(x_valid_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "size = x_train_nn.shape[1]\n",
    "# Prebuit KAGGLE Kernel\n",
    "np.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=prebuilt_nn, epochs=5, batch_size=50, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "pipeline.fit(x_train_nn, y_train)\n",
    "models['DNN'] = pipeline\n",
    "\n",
    "#print(mean_absolute_error(y_valid, pipeline.predict(x_valid_nn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    " \n",
    "#x_train = x_train.values\n",
    "#x_valid = x_valid.values\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "x_train_lstm = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "#x_valid_lstm = x_valid.values.reshape((x_valid.shape[0], 1, x_valid.shape[1]))\n",
    " \n",
    "# design network\n",
    "lstm = Sequential()\n",
    "lstm.add(LSTM(50, input_shape=(x_train_lstm.shape[1], x_train_lstm.shape[2])))\n",
    "lstm.add(PReLU())\n",
    "lstm.add(Dropout(.2))\n",
    "lstm.add(Dense(units = 100 , kernel_initializer = 'normal'))\n",
    "lstm.add(PReLU())\n",
    "lstm.add(Dropout(.2))\n",
    "lstm.add(Dense(units = 50 , kernel_initializer = 'normal'))\n",
    "lstm.add(PReLU())\n",
    "lstm.add(Dense(1))\n",
    "lstm.compile(loss='mae', optimizer='adam')\n",
    "# fit network\n",
    "#validation_data=(x_valid_lstm, y_valid)\n",
    "lstm.fit(x_train_lstm, y_train, epochs=15, batch_size=50, verbose=1, shuffle=False)\n",
    " \n",
    "# make a prediction\n",
    "#yhat = lstm.predict(x_valid_lstm)\n",
    "models['LSTM'] = lstm\n",
    "#mae = mean_absolute_error(y_valid, yhat)\n",
    "#print('Test MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/dnc1994/Kaggle-Playground/blob/master/home-depot/ensemble.py\n",
    "import time\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, \\\n",
    "        ExtraTreesRegressor, AdaBoostClassifier\n",
    "from sklearn import grid_search\n",
    "\n",
    "def mean_absolute_error_(ground_truth, predictions):\n",
    "    return mean_absolute_error(ground_truth, predictions)\n",
    "\n",
    "MAE = make_scorer(mean_absolute_error_, greater_is_better=False)\n",
    "\n",
    "params_xgb = {\n",
    "    'max_depth':        5,  # shuld be 0.5 to 1% of the examples\n",
    "    'subsample':        1,  # Ratio of observations to be used as samples for each tree\n",
    "    'min_child_weight': 10, # Deals with imbalanced data and prevents overfitting as the value >\n",
    "    'objective':        'reg:linear',\n",
    "    'n_estimators':     1000, # Sequential trees to be modelled.\n",
    "    'eta':              0.1,  # Shrinkage. Typically between 0.1 - 0.2 - learning rate for gradient boost (D:0.3)\n",
    "    'eval_metric':      'mae'\n",
    "}\n",
    "\n",
    "class Ensemble(object):\n",
    "    \n",
    "    def __init__(self, n_folds, base_models, floor_models, final_model, include_features, cvgrid, stacker=None):\n",
    "        self.n_folds = n_folds\n",
    "        self.stacker = stacker\n",
    "        self.final_model = final_model\n",
    "        self.base_models = base_models\n",
    "        self.floor_models = floor_models\n",
    "        self.features = include_features\n",
    "        self.param_grid = cvgrid\n",
    "    \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        folds = list(KFold(len(y), n_folds=self.n_folds, shuffle=True, random_state=2016))\n",
    "        S_train = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i, c in enumerate(self.base_models):\n",
    "            print('Fitting For Base Model {} ---'.format(c))       \n",
    "            clf = self.base_models[c]\n",
    "            \n",
    "            for j, (train_idx, test_idx) in enumerate(folds):\n",
    "                print('--- Fitting For Fold %d / %d ---', j + 1, self.n_folds)\n",
    "                X_train = X[train_idx]\n",
    "                y_train = y[train_idx]\n",
    "                X_holdout = X[test_idx]\n",
    "                \n",
    "                if c not in ['XGB', 'LightGBM', 'LSTM']:\n",
    "                    \n",
    "                    clf.fit(X_train, y_train)\n",
    "                    y_pred = clf.predict(X_holdout)[:]\n",
    "                    \n",
    "                    S_train[test_idx, i] = y_pred\n",
    "                    \n",
    "                elif c in ['LSTM']:\n",
    "                    x_train_lstm = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "\n",
    "                    clf.fit(x_train_lstm, y_train, epochs=15, batch_size=50, verbose=1, shuffle=False)\n",
    "                    y_pred = clf.predict(X_holdout.reshape((X_holdout.shape[0], 1, X_holdout.shape[1])))[:]\n",
    "                    \n",
    "                    S_train[test_idx, i] = [i[0] for i in y_pred]\n",
    "\n",
    "                else:\n",
    "                    d_train = xg.DMatrix(X_train, label=y_train, missing=-1)\n",
    "                    d_valid = xg.DMatrix(X_holdout, missing=-1)\n",
    "                    \n",
    "                    clf = xg.train(params_xgb, d_train)\n",
    "                    y_pred = clf.predict(d_valid)[:]\n",
    "                    \n",
    "                    S_train[test_idx, i] = y_pred\n",
    "                    \n",
    "                print('Elapsed: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "            print('Elapsed: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "        print('--- Base Models Trained: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "        if self.features:\n",
    "            S_train = np.append(X, S_train, 1)\n",
    "        \n",
    "        #if self.stacker is None:\n",
    "        #    self.base_models = self.floor_models\n",
    "        #    self.stacker = self.final_model\n",
    "        #    self.fit(S_train, y)\n",
    "\n",
    "        d_train = xg.DMatrix(S_train, label=y, missing=-1)\n",
    "        #d_valid = xg.DMatrix(x_valid, label=y_valid, missing=-1)\n",
    "        xgb_gs = xg.train(params_xgb, d_train, num_boost_round=250, verbose_eval=50)\n",
    "\n",
    "        #else:           \n",
    "        #    grid = grid_search.GridSearchCV(estimator=self.stacker, param_grid=self.param_grid, n_jobs=1, cv=5, verbose=20, scoring=MAE)\n",
    "        #    grid.fit(S_train, y)\n",
    "        \n",
    "        #try:\n",
    "        #    print('Best Params:')\n",
    "        #    print(grid.best_params_)\n",
    "        #    print('Best CV Score:')\n",
    "        #    print(-grid.best_score_)\n",
    "        #    print('Best estimator:')\n",
    "        #    print(grid.best_estimator_)\n",
    "        #except:\n",
    "        #    pass\n",
    "        \n",
    "        self.stacker = xgb_gs#grid.best_estimator_\n",
    "        \n",
    "        print('--- Stacker Trained: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        folds = list(KFold(len(X), n_folds=self.n_folds, shuffle=True, random_state=2016))\n",
    "        if self.features:\n",
    "            S_test = np.append(X, np.zeros((X.shape[0], len(self.base_models))), 1)  \n",
    "            print('Using features of shape', S_test.shape)\n",
    "        else:\n",
    "            S_test = np.zeros((X.shape[0], len(self.base_models)))\n",
    "            print('Using features of shape', S_test.shape)\n",
    "\n",
    "        for ind, c in enumerate(self.base_models):\n",
    "            clf = self.base_models[c]\n",
    "            \n",
    "            # Uses all features.\n",
    "            if self.features:\n",
    "                i = X.shape[1] + ind\n",
    "            else:\n",
    "                i = ind\n",
    "                \n",
    "            S_test_i = np.zeros((X.shape[0], len(folds)))\n",
    "            print('--- Predicting For  #{}'.format(c))\n",
    "            \n",
    "            # Makes predictions for each model\n",
    "            for j, (train_idx, test_idx) in enumerate(folds):    \n",
    "                if c not in ['XGB', 'LSTM']:\n",
    "                    S_test_i[:, j] = clf.predict(X)[:]\n",
    "                    \n",
    "                elif c in ['LSTM']:\n",
    "                    S_test_i[:, j] = [i for i in clf.predict(X.reshape((X.shape[0], 1, X.shape[1])))[:]]\n",
    "                    \n",
    "                else:\n",
    "                    S_test_i[:, j] = clf.predict(X)[:]\n",
    "                \n",
    "            S_test[:, i] = S_test_i.mean(1)\n",
    "\n",
    "        clf = self.stacker\n",
    "        try:\n",
    "            y_pred = clf.predict(S_test)[:]\n",
    "        except:\n",
    "            y_pred = clf.predict(xg.DMatrix(S_test))\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    \n",
    "    def fit_predict(self, X, y, T):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        T = np.array(T)\n",
    "\n",
    "        start_time = time.time()\n",
    "        folds = list(KFold(len(y), n_folds=self.n_folds, shuffle=True, random_state=2016))\n",
    "\n",
    "        S_train = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        S_test  = np.zeros((T.shape[0], len(self.base_models)))\n",
    "\n",
    "        for i, c in enumerate(self.base_models):\n",
    "            print('########## \\nFitting For Base Model {} \\n##########'.format(c))\n",
    "            clf = self.base_models[c]\n",
    "            S_test_i = np.zeros((T.shape[0], len(folds)))\n",
    "\n",
    "            for j, (train_idx, test_idx) in enumerate(folds):\n",
    "                print('--- Fitting For Fold #{0} / {1} ---'.format(j+1, self.n_folds))\n",
    "                X_train = X[train_idx]\n",
    "                y_train = y[train_idx]\n",
    "                X_holdout = X[test_idx]\n",
    "                \n",
    "                if c not in ['XGB', 'LightGBM', 'LSTM']:\n",
    "                    clf.fit(X_train, y_train)\n",
    "                    y_pred = clf.predict(X_holdout)[:]\n",
    "                    \n",
    "                    S_train[test_idx, i] = y_pred\n",
    "                    S_test_i[:, j] = clf.predict(T)[:]\n",
    "                    \n",
    "                elif c in ['LSTM']:\n",
    "                    x_train_lstm = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "                    \n",
    "                    clf.fit(x_train_lstm, y_train, epochs=15, batch_size=50, verbose=1, shuffle=False)\n",
    "                    y_pred = clf.predict(X_holdout.reshape((X_holdout.shape[0], 1, X_holdout.shape[1])))[:]\n",
    "                    \n",
    "                    S_train[test_idx, i] = [i[0] for i in y_pred]\n",
    "                    S_test_i[:, j] = [i for i in clf.predict(T.reshape((T.shape[0], 1, T.shape[1])))[:]]\n",
    "                    \n",
    "                else:\n",
    "                    d_train = xg.DMatrix(X_train, label=y_train, missing=-1)\n",
    "                    d_valid = xg.DMatrix(X_holdout, missing=-1)\n",
    "                    \n",
    "                    clf = xg.train(params_xgb, d_train)\n",
    "                    y_pred = clf.predict(d_valid)[:]\n",
    "                    \n",
    "                    S_train[test_idx, i] = y_pred\n",
    "                    data_pred = xg.DMatrix(T, missing=-1)\n",
    "                    S_test_i[:, j] = clf.predict(data_pred)[:]\n",
    "\n",
    "                print('Elapsed: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "            S_test[:, i] = S_test_i.mean(1)\n",
    "            print('Elapsed: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "        print('--- Base Models Trained: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "        param_grid = {'n_estimators':  [100],\n",
    "                      'learning_rate': [0.05],\n",
    "                      'subsample':     [0.75]}\n",
    "\n",
    "        #grid = grid_search.GridSearchCV(estimator=self.stacker, param_grid=param_grid, n_jobs=1, cv=5, verbose=20, scoring=MAE)\n",
    "        \n",
    "        \n",
    "        grid.fit(S_train, y)\n",
    "\n",
    "        try:\n",
    "            print('Param grid:')\n",
    "            print(param_grid)\n",
    "            print('Best Params:')\n",
    "            print(grid.best_params_)\n",
    "            print('Best CV Score:')\n",
    "            print(-grid.best_score_)\n",
    "            print('Best estimator:')\n",
    "            print(grid.best_estimator_)\n",
    "            print(message)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        print('--- Stacker Trained: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "        y_pred = grid.predict(S_test)[:]\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del models['CatBoost']\n",
    "#del models['XGB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators':  [50, 100, 150],\n",
    "              'learning_rate': [0.03, 0.05, 0.07],\n",
    "              'subsample':     [0.5, 0.75, 1]\n",
    "             }\n",
    "\n",
    "stackers = {'GBM' : GradientBoostingRegressor(),}\n",
    "\n",
    "ensemble = Ensemble(n_folds=5,\n",
    "                    base_models=models, \n",
    "                    floor_models=stackers, \n",
    "                    final_model=models['DNN'],\n",
    "                    include_features=False, \n",
    "                    cvgrid=param_grid)\n",
    "                    \n",
    "#model_ensemble = ensemble.fit_predict(x_train[:100], y_train[:100], x_valid)\n",
    "# MAE 0.0653212760898 - lr 0.03, nest = 50, subsample: 0.5\n",
    "ensemble.fit(X_train, y_train)\n",
    "#final_prediction = ensemble.predict(X_test2016)\n",
    "#print('MAE', mean_absolute_error(y_valid, final_prediction))\n",
    "\n",
    "#del final_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(X, K, randomise = False):\n",
    "    \"\"\"Generates K (training, validation) pairs from the items in X.\"\"\"\n",
    "    \n",
    "    if randomise: from random import shuffle; X=list(X); shuffle(X)\n",
    "    for k in range(K):\n",
    "        training   = [x for i, x in enumerate(X) if i % K != k]\n",
    "        validation = [x for i, x in enumerate(X) if i % K == k]\n",
    "        \n",
    "        yield training, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "for training, validation in k_fold_cross_validation(x_train_data.values, K=5):\n",
    "    pred_k = ensemble.predict(validation)\n",
    "    print('MAE',mean_absolute_error(x_train_label[index:index+len(validation)], pred_k))\n",
    "    index += len(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## LAYER 1 ##########\n",
    "# Submodel  1 : OLS                      # Ordinary least squares estimator Sklearn implementation\n",
    "# Submodel  2 : BR                       # Bayesian ridge regression - Sklearn implementation\n",
    "# Submodel  3 : DNN                      # Dense Neural Network - Keras - Dense layers \n",
    "# Submodel  4 : LightGBM                 # Light Gradient Boosting - https://github.com/Microsoft/LightGBM\n",
    "# Submodel  5 : XGBoost                  # Extreme Gradient Boosting - http://xgboost.readthedocs.io/en/latest/model.html\n",
    "# Submodel  6 : CatBoost                 # Categorical Boosting https://github.com/catboost/catboost\n",
    "# Submodel  7 : LSTM                     # Long Short Term Memory Neural Network - Keras implementation\n",
    "# Submodel  8 : RandomForestRegressor    # Sklearn implementation\n",
    "# Submodel  9 : ExtraTreesRegressor      # Sklearn implementation\n",
    "# Submodel 10 : SVR                      # Support vector machines for regression - Sklearn implementation\n",
    "# Submodel 11 : AdaBoost                 # Adaptive Boosting Sklearn Implementation\n",
    "\n",
    "########## LAYER 2 ##########\n",
    "# https://www.kaggle.com/dragost/boosted-trees-lb-0-0643707/edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting already build test framework for 2017 and 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dates = {\n",
    "    '201610': pd.Timestamp('2016-09-30'),\n",
    "    '201611': pd.Timestamp('2016-10-31'),\n",
    "    '201612': pd.Timestamp('2016-11-30'),\n",
    "    '201710': pd.Timestamp('2017-09-30'),\n",
    "    '201711': pd.Timestamp('2017-10-31'),\n",
    "    '201712': pd.Timestamp('2017-11-30')\n",
    "}\n",
    "\n",
    "for d in test_dates.keys():\n",
    "    if '2016' in d:\n",
    "        print('Predicting for 2016')\n",
    "        X_test2016['month'] = int(d[-2:])\n",
    "        sample_submission[d] = predict(ensemble, X_test2016)\n",
    "    elif '2017' in d:\n",
    "        print('Predicting for 2017')\n",
    "        X_test2017['month'] = int(d[-2:]) + 12\n",
    "        sample_submission[d] = predict(ensemble, X_test2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train['month'] = (pd.to_datetime(train['transactiondate']).dt.year - 2016)*12 + pd.to_datetime(train['transactiondate']).dt.month\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Building properties data')\n",
    "properties2017 = pd.read_csv('../Data/properties_2017.csv', low_memory = False)\n",
    "sample_prediction = pd.merge(sample['ParcelId'].to_frame(), properties2017, how='left', left_on=['ParcelId'], right_on=['parcelid'])\n",
    "#transactions[['propertycountylandusecode', 'propertyzoningdesc']] = transactions[['propertycountylandusecode', 'propertyzoningdesc']].astype(str).apply(LabelEncoder().fit_transform)\n",
    "sample_prediction['taxdelinquencyflag'].replace('Y', 1, inplace=True)\n",
    "sample_prediction.drop(to_drop, axis=1, inplace=True)\n",
    "sample_prediction = complex_features(sample_prediction)\n",
    "sample_prediction.drop(['parcelid', 'propertyzoningdesc', 'propertycountylandusecode', 'fireplacecnt'], axis=1, inplace=True)\n",
    "sample_prediction.fillna(sample_prediction.median(), inplace = True)\n",
    "\n",
    "del properties2017\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/zillow-prize-1/discussion/33899, Oct,Nov,Dec\n",
    "\n",
    "WEIGHT_XGB = 0.4\n",
    "WEIGHT_CAT = 0.6\n",
    "\n",
    "test_dates = {\n",
    "    '201610': pd.Timestamp('2016-09-30'),\n",
    "    '201611': pd.Timestamp('2016-10-31'),\n",
    "    '201612': pd.Timestamp('2016-11-30'),\n",
    "    '201710': pd.Timestamp('2017-09-30'),\n",
    "    '201711': pd.Timestamp('2017-10-31'),\n",
    "    '201712': pd.Timestamp('2017-11-30')\n",
    "}\n",
    "\n",
    "for m in test_dates.keys():\n",
    "    \n",
    "    print('Processing', m)\n",
    "    sample_prediction['transactiondate'] = test_dates[m]\n",
    "    sample_prediction = time_data(sample_prediction)\n",
    "    \n",
    "    print('Ensemble Prediction', m)\n",
    "    sample_prediction['ensemble'] = ensemble.predict(sample_prediction[best_columns])\n",
    "    \n",
    "    print('XGB - CatBoost Train', m)\n",
    "    predictions_xgb = xgb_gs.predict(xg.DMatrix(sample_prediction[list(best_columns) + ['ensemble']]))\n",
    "    predictions_cat = get_cat_boost_all(sample_train, sample_label, sample_prediction[list(best_columns) + ['ensemble']])\n",
    "\n",
    "    sample[m] = (WEIGHT_XGB * predictions_xgb) + (WEIGHT_CAT * predictions_cat)\n",
    "    \n",
    "    del predictions_xgb, predictions_cat\n",
    "    gc.collect()\n",
    "    \n",
    "#del x_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2017 = pd.read_csv('../Data/train_2017.csv', parse_dates=['transactiondate'], low_memory=False)\n",
    "sample_train = pd.merge(train2017, sample_prediction, how='left', left_on='parcelid' ,right_on='ParcelId')\n",
    "sample_train['ensemble'] = ensemble.predict(sample_train[best_columns])\n",
    "sample_label = sample_train['logerror']\n",
    "sample_train = time_data(sample_train)[list(best_columns) + ['ensemble']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train2017\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_xgb = {\n",
    "    'max_depth':        5,  # shuld be 0.5 to 1% of the examples\n",
    "    'subsample':        1,  # Ratio of observations to be used as samples for each tree\n",
    "    'min_child_weight': 10, # Deals with imbalanced data and prevents overfitting as the value >\n",
    "    'objective':        'reg:linear',\n",
    "    'n_estimators':     1000, # Sequential trees to be modelled.\n",
    "    'eta':              0.1,  # Shrinkage. Typically between 0.1 - 0.2 - learning rate for gradient boost (D:0.3)\n",
    "    'eval_metric':      'mae'\n",
    "}\n",
    "\n",
    "d_train = xg.DMatrix(sample_train, label=sample_label)\n",
    "xgb_gs = xg.train(params_xgb, d_train, num_boost_round=250, verbose_eval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cat_boost_all(x_train, y_train, x_valid):\n",
    "    num_ensembles = 5\n",
    "    y_pred_valid = 0.0\n",
    "\n",
    "    print('Initialising CAT Boost Regression')\n",
    "    for i in tqdm(range(num_ensembles)):\n",
    "        print('Building ensemble', i)\n",
    "        # Use CV, tune hyperparameters\n",
    "        catb = CatBoostRegressor(\n",
    "                iterations=600, learning_rate=0.03,\n",
    "                depth=5, l2_leaf_reg=3,\n",
    "                loss_function='MAE',\n",
    "                eval_metric='MAE',\n",
    "                random_seed=i)\n",
    "\n",
    "        catb.fit(x_train, y_train, cat_features=cat_index)\n",
    "\n",
    "        y_pred_valid += catb.predict(x_valid)\n",
    "        \n",
    "    y_pred_valid /= num_ensembles\n",
    "    \n",
    "    return y_pred_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prediction.to_csv('submission5.csv',index=False)\n",
    "sample_prediction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RFE"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model_lr = LinearRegression()\n",
    "model_xgb = xg.XGBRegressor()\n",
    "selector = RFE(model_xgb, 100, step=500)\n",
    "model = selector.estimator.fit(x_train, y_train)\n",
    "\n",
    "dict_features = plot_best_features(model, data=x_all, num_features=100, figsize=(5,15))\n",
    "\n",
    "best_columns = list(dict_features.keys())\n",
    "new_sparse_columns = x_all.columns\n",
    "x_train = pd.DataFrame(x_train, columns=x_all.columns)[best_columns].values\n",
    "x_test  = pd.DataFrame(x_test,  columns=x_all.columns)[best_columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
