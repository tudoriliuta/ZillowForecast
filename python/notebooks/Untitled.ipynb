{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading data from disk ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (22,32,34,49,55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing data for LightGBM ...\n",
      "(90275, 53) (90275,)\n",
      "\n",
      "Fitting LightGBM model ...\n",
      "\n",
      "Prepare for LightGBM prediction ...\n",
      "   Read sample file ...\n",
      "   ...\n",
      "   Merge with property data ...\n",
      "   ...\n",
      "   ...\n",
      "   ...\n",
      "   Preparing x_test...\n",
      "   ...\n",
      "\n",
      "Start LightGBM prediction ...\n",
      "\n",
      "Unadjusted LightGBM predictions:\n",
      "          0\n",
      "0  0.031841\n",
      "1  0.031620\n",
      "2  0.035182\n",
      "3  0.029522\n",
      "4  0.023094\n",
      "\n",
      "Re-reading properties file ...\n",
      "\n",
      "Processing data for XGBoost ...\n",
      "Shape train: (90275, 57)\n",
      "Shape test: (2985217, 57)\n",
      "After removing outliers:\n",
      "Shape train: (88528, 57)\n",
      "Shape test: (2985217, 57)\n",
      "\n",
      "Setting up data for XGBoost ...\n",
      "num_boost_rounds=250\n",
      "\n",
      "Training XGBoost ...\n",
      "\n",
      "Predicting with XGBoost ...\n",
      "\n",
      "First XGBoost predictions:\n",
      "          0\n",
      "0 -0.023582\n",
      "1 -0.032242\n",
      "2  0.007546\n",
      "3  0.064200\n",
      "4 -0.004868\n",
      "\n",
      "Setting up data for XGBoost ...\n",
      "num_boost_rounds=150\n",
      "\n",
      "Training XGBoost again ...\n",
      "\n",
      "Predicting with XGBoost again ...\n",
      "\n",
      "Second XGBoost predictions:\n",
      "          0\n",
      "0 -0.073580\n",
      "1 -0.010724\n",
      "2  0.030864\n",
      "3  0.078579\n",
      "4  0.022030\n",
      "\n",
      "Combined XGBoost predictions:\n",
      "          0\n",
      "0 -0.033581\n",
      "1 -0.027938\n",
      "2  0.012210\n",
      "3  0.067076\n",
      "4  0.000511\n",
      "\n",
      "\n",
      "Processing data for Neural Network ...\n",
      "\n",
      "Loading train, prop and sample data...\n",
      "Fitting Label Encoder on properties...\n",
      "Creating training set...\n",
      "Filling NA/NaN values...\n",
      "Creating x_train and y_train from df_train...\n",
      "(90275, 56) (90275,)\n",
      "Creating df_test...\n",
      "Merging Sample with property data...\n",
      "Shape of x_test: (2985217, 56)\n",
      "Preparing x_test...\n",
      "\n",
      "Preprocessing neural network data...\n",
      "len_x is: 56\n",
      "\n",
      "Setting up neural network model...\n",
      "\n",
      "Fitting neural network model...\n",
      "Epoch 1/70\n",
      "20s - loss: 0.0711\n",
      "Epoch 2/70\n",
      "18s - loss: 0.0682\n",
      "Epoch 3/70\n",
      "18s - loss: 0.0682\n",
      "Epoch 4/70\n",
      "23s - loss: 0.0681\n",
      "Epoch 5/70\n",
      "19s - loss: 0.0680\n",
      "Epoch 6/70\n",
      "20s - loss: 0.0680\n",
      "Epoch 7/70\n",
      "20s - loss: 0.0679\n",
      "Epoch 8/70\n",
      "23s - loss: 0.0679\n",
      "Epoch 9/70\n",
      "21s - loss: 0.0678\n",
      "Epoch 10/70\n",
      "20s - loss: 0.0678\n",
      "Epoch 11/70\n",
      "19s - loss: 0.0677\n",
      "Epoch 12/70\n",
      "19s - loss: 0.0677\n",
      "Epoch 13/70\n",
      "19s - loss: 0.0677\n",
      "Epoch 14/70\n",
      "20s - loss: 0.0677\n",
      "Epoch 15/70\n",
      "19s - loss: 0.0676\n",
      "Epoch 16/70\n",
      "19s - loss: 0.0676\n",
      "Epoch 17/70\n",
      "19s - loss: 0.0676\n",
      "Epoch 18/70\n",
      "19s - loss: 0.0676\n",
      "Epoch 19/70\n",
      "19s - loss: 0.0676\n",
      "Epoch 20/70\n",
      "19s - loss: 0.0676\n",
      "Epoch 21/70\n",
      "19s - loss: 0.0675\n",
      "Epoch 22/70\n",
      "19s - loss: 0.0675\n",
      "Epoch 23/70\n",
      "19s - loss: 0.0675\n",
      "Epoch 24/70\n",
      "21s - loss: 0.0675\n",
      "Epoch 25/70\n",
      "22s - loss: 0.0674\n",
      "Epoch 26/70\n",
      "21s - loss: 0.0675\n",
      "Epoch 27/70\n",
      "20s - loss: 0.0675\n",
      "Epoch 28/70\n",
      "20s - loss: 0.0674\n",
      "Epoch 29/70\n",
      "19s - loss: 0.0675\n",
      "Epoch 30/70\n",
      "21s - loss: 0.0675\n",
      "Epoch 31/70\n",
      "20s - loss: 0.0674\n",
      "Epoch 32/70\n",
      "19s - loss: 0.0674\n",
      "Epoch 33/70\n",
      "19s - loss: 0.0674\n",
      "Epoch 34/70\n",
      "19s - loss: 0.0674\n",
      "Epoch 35/70\n",
      "19s - loss: 0.0674\n",
      "Epoch 36/70\n",
      "19s - loss: 0.0674\n",
      "Epoch 37/70\n",
      "19s - loss: 0.0674\n",
      "Epoch 38/70\n",
      "19s - loss: 0.0674\n",
      "Epoch 39/70\n",
      "19s - loss: 0.0674\n",
      "Epoch 40/70\n",
      "19s - loss: 0.0674\n",
      "Epoch 41/70\n",
      "19s - loss: 0.0673\n",
      "Epoch 42/70\n",
      "19s - loss: 0.0674\n",
      "Epoch 43/70\n",
      "19s - loss: 0.0674\n",
      "Epoch 44/70\n",
      "19s - loss: 0.0673\n",
      "Epoch 45/70\n",
      "19s - loss: 0.0674\n",
      "Epoch 46/70\n",
      "19s - loss: 0.0673\n",
      "Epoch 47/70\n",
      "19s - loss: 0.0673\n",
      "Epoch 48/70\n",
      "19s - loss: 0.0673\n",
      "Epoch 49/70\n",
      "19s - loss: 0.0673\n",
      "Epoch 50/70\n",
      "19s - loss: 0.0673\n",
      "Epoch 51/70\n",
      "19s - loss: 0.0673\n",
      "Epoch 52/70\n",
      "19s - loss: 0.0673\n",
      "Epoch 53/70\n",
      "19s - loss: 0.0673\n",
      "Epoch 54/70\n",
      "19s - loss: 0.0673\n",
      "Epoch 55/70\n",
      "23s - loss: 0.0673\n",
      "Epoch 56/70\n",
      "19s - loss: 0.0673\n",
      "Epoch 57/70\n",
      "19s - loss: 0.0673\n",
      "Epoch 58/70\n",
      "21s - loss: 0.0673\n",
      "Epoch 59/70\n",
      "21s - loss: 0.0673\n",
      "Epoch 60/70\n",
      "20s - loss: 0.0673\n",
      "Epoch 61/70\n",
      "19s - loss: 0.0673\n",
      "Epoch 62/70\n",
      "21s - loss: 0.0673\n",
      "Epoch 63/70\n",
      "22s - loss: 0.0672\n",
      "Epoch 64/70\n",
      "20s - loss: 0.0672\n",
      "Epoch 65/70\n",
      "19s - loss: 0.0672\n",
      "Epoch 66/70\n",
      "21s - loss: 0.0672\n",
      "Epoch 67/70\n",
      "20s - loss: 0.0672\n",
      "Epoch 68/70\n",
      "20s - loss: 0.0672\n",
      "Epoch 69/70\n",
      "20s - loss: 0.0672\n",
      "Epoch 70/70\n",
      "21s - loss: 0.0672\n",
      "\n",
      "Predicting with neural network model...\n",
      "\n",
      "Preparing results for write...\n",
      "Type of nn_pred is  <class 'numpy.ndarray'>\n",
      "Shape of nn_pred is  (2985217,)\n",
      "\n",
      "Neural Network predictions:\n",
      "          0\n",
      "0 -0.012359\n",
      "1 -0.013587\n",
      "2  0.784678\n",
      "3  0.128345\n",
      "4  0.126260\n",
      "\n",
      "\n",
      "Processing data for OLS ...\n",
      "90275 2985217 2985217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:436: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:437: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:438: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:439: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting OLS...\n",
      "fit...\n",
      "0.068370088877\n",
      "\n",
      "Combining XGBoost, LightGBM, NN, and baseline predicitons ...\n",
      "\n",
      "Combined XGB/LGB/NN/baseline predictions:\n",
      "          0\n",
      "0 -0.015388\n",
      "1 -0.011817\n",
      "2  0.083668\n",
      "3  0.062580\n",
      "4  0.016842\n",
      "\n",
      "Predicting with OLS and combining with XGB/LGB/NN/baseline predicitons: ...\n",
      "predict... 0\n",
      "predict... 1\n",
      "predict... 2\n",
      "predict... 3\n",
      "predict... 4\n",
      "predict... 5\n",
      "\n",
      "Combined XGB/LGB/NN/baseline/OLS predictions:\n",
      "   ParcelId  201610  201611  201612  201710  201711  201712\n",
      "0  10754147 -0.0185 -0.0185 -0.0185 -0.0185 -0.0185 -0.0185\n",
      "1  10759547 -0.0151 -0.0151 -0.0151 -0.0151 -0.0151 -0.0151\n",
      "2  10843547  0.1329  0.1329  0.1329  0.1329  0.1329  0.1329\n",
      "3  10859147  0.0684  0.0684  0.0684  0.0684  0.0684  0.0684\n",
      "4  10879947  0.0198  0.0198  0.0198  0.0198  0.0198  0.0198\n",
      "\n",
      "Writing results to disk ...\n",
      "\n",
      "Finished ...\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "FUDGE_FACTOR = 1.1200  # Multiply forecasts by this\n",
    "\n",
    "XGB_WEIGHT = 0.6200\n",
    "BASELINE_WEIGHT = 0.0100\n",
    "OLS_WEIGHT = 0.0620\n",
    "NN_WEIGHT = 0.0800\n",
    "\n",
    "XGB1_WEIGHT = 0.8000  # Weight of first in combination of two XGB models\n",
    "\n",
    "BASELINE_PRED = 0.0115   # Baseline based on mean of training data, per Oleg\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import random\n",
    "import datetime as dt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout, BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.noise import GaussianDropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "\n",
    "\n",
    "##### READ IN RAW DATA\n",
    "\n",
    "print( \"\\nReading data from disk ...\")\n",
    "prop = pd.read_csv('../Data/properties_2016.csv')\n",
    "train = pd.read_csv(\"../Data/train_2016_v2.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################\n",
    "################\n",
    "##  LightGBM  ##\n",
    "################\n",
    "################\n",
    "\n",
    "# This section is (I think) originally derived from SIDHARTH's script:\n",
    "#   https://www.kaggle.com/sidharthkumar/trying-lightgbm\n",
    "# which was forked and tuned by Yuqing Xue:\n",
    "#   https://www.kaggle.com/yuqingxue/lightgbm-85-97\n",
    "# and updated by me (Andy Harless):\n",
    "#   https://www.kaggle.com/aharless/lightgbm-with-outliers-remaining\n",
    "# and a lot of additional changes have happened since then\n",
    "\n",
    "##### PROCESS DATA FOR LIGHTGBM\n",
    "\n",
    "print( \"\\nProcessing data for LightGBM ...\" )\n",
    "for c, dtype in zip(prop.columns, prop.dtypes):\t\n",
    "    if dtype == np.float64:\t\t\n",
    "        prop[c] = prop[c].astype(np.float32)\n",
    "\n",
    "df_train = train.merge(prop, how='left', on='parcelid')\n",
    "df_train.fillna(df_train.median(),inplace = True)\n",
    "\n",
    "x_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', \n",
    "                         'propertycountylandusecode', 'fireplacecnt', 'fireplaceflag'], axis=1)\n",
    "#x_train['Ratio_1'] = x_train['taxvaluedollarcnt']/x_train['taxamount']\n",
    "y_train = df_train['logerror'].values\n",
    "print(x_train.shape, y_train.shape)\n",
    "\n",
    "\n",
    "train_columns = x_train.columns\n",
    "\n",
    "for c in x_train.dtypes[x_train.dtypes == object].index.values:\n",
    "    x_train[c] = (x_train[c] == True)\n",
    "\n",
    "del df_train; gc.collect()\n",
    "\n",
    "x_train = x_train.values.astype(np.float32, copy=False)\n",
    "d_train = lgb.Dataset(x_train, label=y_train)\n",
    "\n",
    "\n",
    "\n",
    "##### RUN LIGHTGBM\n",
    "\n",
    "params = {}\n",
    "params['max_bin'] = 10\n",
    "params['learning_rate'] = 0.0021 # shrinkage_rate\n",
    "params['boosting_type'] = 'gbdt'\n",
    "params['objective'] = 'regression'\n",
    "params['metric'] = 'l1'          # or 'mae'\n",
    "params['sub_feature'] = 0.345    # feature_fraction (small values => use very different submodels)\n",
    "params['bagging_fraction'] = 0.85 # sub_row\n",
    "params['bagging_freq'] = 40\n",
    "params['num_leaves'] = 512        # num_leaf\n",
    "params['min_data'] = 500         # min_data_in_leaf\n",
    "params['min_hessian'] = 0.05     # min_sum_hessian_in_leaf\n",
    "params['verbose'] = 0\n",
    "params['feature_fraction_seed'] = 2\n",
    "params['bagging_seed'] = 3\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "print(\"\\nFitting LightGBM model ...\")\n",
    "clf = lgb.train(params, d_train, 430)\n",
    "\n",
    "del d_train; gc.collect()\n",
    "del x_train; gc.collect()\n",
    "\n",
    "print(\"\\nPrepare for LightGBM prediction ...\")\n",
    "print(\"   Read sample file ...\")\n",
    "sample = pd.read_csv('../Data/sample_submission.csv')\n",
    "print(\"   ...\")\n",
    "sample['parcelid'] = sample['ParcelId']\n",
    "print(\"   Merge with property data ...\")\n",
    "df_test = sample.merge(prop, on='parcelid', how='left')\n",
    "print(\"   ...\")\n",
    "del sample, prop; gc.collect()\n",
    "print(\"   ...\")\n",
    "#df_test['Ratio_1'] = df_test['taxvaluedollarcnt']/df_test['taxamount']\n",
    "x_test = df_test[train_columns]\n",
    "print(\"   ...\")\n",
    "del df_test; gc.collect()\n",
    "print(\"   Preparing x_test...\")\n",
    "for c in x_test.dtypes[x_test.dtypes == object].index.values:\n",
    "    x_test[c] = (x_test[c] == True)\n",
    "print(\"   ...\")\n",
    "x_test = x_test.values.astype(np.float32, copy=False)\n",
    "\n",
    "print(\"\\nStart LightGBM prediction ...\")\n",
    "p_test = clf.predict(x_test)\n",
    "\n",
    "del x_test; gc.collect()\n",
    "\n",
    "print( \"\\nUnadjusted LightGBM predictions:\" )\n",
    "print( pd.DataFrame(p_test).head() )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################\n",
    "################\n",
    "##  XGBoost   ##\n",
    "################\n",
    "################\n",
    "\n",
    "# This section is (I think) originally derived from Infinite Wing's script:\n",
    "#   https://www.kaggle.com/infinitewing/xgboost-without-outliers-lb-0-06463\n",
    "# inspired by this thread:\n",
    "#   https://www.kaggle.com/c/zillow-prize-1/discussion/33710\n",
    "# but the code has gone through a lot of changes since then\n",
    "\n",
    "\n",
    "##### RE-READ PROPERTIES FILE\n",
    "##### (I tried keeping a copy, but the program crashed.)\n",
    "\n",
    "print( \"\\nRe-reading properties file ...\")\n",
    "properties = pd.read_csv('../Data/properties_2016.csv')\n",
    "\n",
    "\n",
    "\n",
    "##### PROCESS DATA FOR XGBOOST\n",
    "\n",
    "print( \"\\nProcessing data for XGBoost ...\")\n",
    "for c in properties.columns:\n",
    "    properties[c]=properties[c].fillna(-1)\n",
    "    if properties[c].dtype == 'object':\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(properties[c].values))\n",
    "        properties[c] = lbl.transform(list(properties[c].values))\n",
    "\n",
    "train_df = train.merge(properties, how='left', on='parcelid')\n",
    "x_train = train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\n",
    "x_test = properties.drop(['parcelid'], axis=1)\n",
    "# shape        \n",
    "print('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))\n",
    "\n",
    "# drop out ouliers\n",
    "train_df=train_df[ train_df.logerror > -0.4 ]\n",
    "train_df=train_df[ train_df.logerror < 0.419 ]\n",
    "x_train=train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\n",
    "y_train = train_df[\"logerror\"].values.astype(np.float32)\n",
    "y_mean = np.mean(y_train)\n",
    "\n",
    "print('After removing outliers:')     \n",
    "print('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### RUN XGBOOST\n",
    "\n",
    "print(\"\\nSetting up data for XGBoost ...\")\n",
    "# xgboost params\n",
    "xgb_params = {\n",
    "    'eta': 0.037,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.80,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'mae',\n",
    "    'lambda': 0.8,   \n",
    "    'alpha': 0.4, \n",
    "    'base_score': y_mean,\n",
    "    'silent': 1\n",
    "}\n",
    "\n",
    "dtrain = xgb.DMatrix(x_train, y_train)\n",
    "dtest = xgb.DMatrix(x_test)\n",
    "\n",
    "num_boost_rounds = 250\n",
    "print(\"num_boost_rounds=\"+str(num_boost_rounds))\n",
    "\n",
    "# train model\n",
    "print( \"\\nTraining XGBoost ...\")\n",
    "model = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n",
    "\n",
    "print( \"\\nPredicting with XGBoost ...\")\n",
    "xgb_pred1 = model.predict(dtest)\n",
    "\n",
    "print( \"\\nFirst XGBoost predictions:\" )\n",
    "print( pd.DataFrame(xgb_pred1).head() )\n",
    "\n",
    "\n",
    "\n",
    "##### RUN XGBOOST AGAIN\n",
    "\n",
    "print(\"\\nSetting up data for XGBoost ...\")\n",
    "# xgboost params\n",
    "xgb_params = {\n",
    "    'eta': 0.033,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.80,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'mae',\n",
    "    'base_score': y_mean,\n",
    "    'silent': 1\n",
    "}\n",
    "\n",
    "num_boost_rounds = 150\n",
    "print(\"num_boost_rounds=\"+str(num_boost_rounds))\n",
    "\n",
    "print( \"\\nTraining XGBoost again ...\")\n",
    "model = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n",
    "\n",
    "print( \"\\nPredicting with XGBoost again ...\")\n",
    "xgb_pred2 = model.predict(dtest)\n",
    "\n",
    "print( \"\\nSecond XGBoost predictions:\" )\n",
    "print( pd.DataFrame(xgb_pred2).head() )\n",
    "\n",
    "\n",
    "\n",
    "##### COMBINE XGBOOST RESULTS\n",
    "xgb_pred = XGB1_WEIGHT*xgb_pred1 + (1-XGB1_WEIGHT)*xgb_pred2\n",
    "#xgb_pred = xgb_pred1\n",
    "\n",
    "print( \"\\nCombined XGBoost predictions:\" )\n",
    "print( pd.DataFrame(xgb_pred).head() )\n",
    "\n",
    "del train_df\n",
    "del x_train\n",
    "del x_test\n",
    "del properties\n",
    "del dtest\n",
    "del dtrain\n",
    "del xgb_pred1\n",
    "del xgb_pred2 \n",
    "gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################\n",
    "######################\n",
    "##  Neural Network  ##\n",
    "######################\n",
    "######################\n",
    "\n",
    "# Neural network copied from this script:\n",
    "#   https://www.kaggle.com/aharless/keras-neural-network-lb-06492 (version 20)\n",
    "# which was built on the skeleton in this notebook:\n",
    "#   https://www.kaggle.com/prasunmishra/ann-using-keras\n",
    "\n",
    "\n",
    "# Read in data for neural network\n",
    "print( \"\\n\\nProcessing data for Neural Network ...\")\n",
    "print('\\nLoading train, prop and sample data...')\n",
    "train = pd.read_csv(\"../Data/train_2016_v2.csv\", parse_dates=[\"transactiondate\"])\n",
    "prop = pd.read_csv('../Data/properties_2016.csv')\n",
    "sample = pd.read_csv('../Data/sample_submission.csv')\n",
    " \n",
    "print('Fitting Label Encoder on properties...')\n",
    "for c in prop.columns:\n",
    "    prop[c]=prop[c].fillna(-1)\n",
    "    if prop[c].dtype == 'object':\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(prop[c].values))\n",
    "        prop[c] = lbl.transform(list(prop[c].values))\n",
    "        \n",
    "print('Creating training set...')\n",
    "df_train = train.merge(prop, how='left', on='parcelid')\n",
    "\n",
    "df_train[\"transactiondate\"] = pd.to_datetime(df_train[\"transactiondate\"])\n",
    "df_train[\"transactiondate_year\"] = df_train[\"transactiondate\"].dt.year\n",
    "df_train[\"transactiondate_month\"] = df_train[\"transactiondate\"].dt.month\n",
    "df_train['transactiondate_quarter'] = df_train['transactiondate'].dt.quarter\n",
    "df_train[\"transactiondate\"] = df_train[\"transactiondate\"].dt.day\n",
    "\n",
    "print('Filling NA/NaN values...' )\n",
    "df_train.fillna(-1.0)\n",
    "\n",
    "print('Creating x_train and y_train from df_train...' )\n",
    "x_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', 'propertycountylandusecode','fireplacecnt', 'fireplaceflag'], axis=1)\n",
    "y_train = df_train[\"logerror\"]\n",
    "\n",
    "y_mean = np.mean(y_train)\n",
    "print(x_train.shape, y_train.shape)\n",
    "train_columns = x_train.columns\n",
    "\n",
    "for c in x_train.dtypes[x_train.dtypes == object].index.values:\n",
    "    x_train[c] = (x_train[c] == True)\n",
    "\n",
    "print('Creating df_test...')\n",
    "sample['parcelid'] = sample['ParcelId']\n",
    "\n",
    "print(\"Merging Sample with property data...\")\n",
    "df_test = sample.merge(prop, on='parcelid', how='left')\n",
    "\n",
    "df_test[\"transactiondate\"] = pd.to_datetime('2016-11-15')  # placeholder value for preliminary version\n",
    "df_test[\"transactiondate_year\"] = df_test[\"transactiondate\"].dt.year\n",
    "df_test[\"transactiondate_month\"] = df_test[\"transactiondate\"].dt.month\n",
    "df_test['transactiondate_quarter'] = df_test['transactiondate'].dt.quarter\n",
    "df_test[\"transactiondate\"] = df_test[\"transactiondate\"].dt.day     \n",
    "x_test = df_test[train_columns]\n",
    "\n",
    "print('Shape of x_test:', x_test.shape)\n",
    "print(\"Preparing x_test...\")\n",
    "for c in x_test.dtypes[x_test.dtypes == object].index.values:\n",
    "    x_test[c] = (x_test[c] == True)\n",
    "\n",
    "    \n",
    "## Preprocessing\n",
    "print(\"\\nPreprocessing neural network data...\")\n",
    "imputer= Imputer()\n",
    "imputer.fit(x_train.iloc[:, :])\n",
    "x_train = imputer.transform(x_train.iloc[:, :])\n",
    "imputer.fit(x_test.iloc[:, :])\n",
    "x_test = imputer.transform(x_test.iloc[:, :])\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "\n",
    "len_x=int(x_train.shape[1])\n",
    "print(\"len_x is:\",len_x)\n",
    "\n",
    "\n",
    "# Neural Network\n",
    "print(\"\\nSetting up neural network model...\")\n",
    "nn = Sequential()\n",
    "nn.add(Dense(units = 400 , kernel_initializer = 'normal', input_dim = len_x))\n",
    "nn.add(PReLU())\n",
    "nn.add(Dropout(.4))\n",
    "nn.add(Dense(units = 160 , kernel_initializer = 'normal'))\n",
    "nn.add(PReLU())\n",
    "nn.add(BatchNormalization())\n",
    "nn.add(Dropout(.6))\n",
    "nn.add(Dense(units = 64 , kernel_initializer = 'normal'))\n",
    "nn.add(PReLU())\n",
    "nn.add(BatchNormalization())\n",
    "nn.add(Dropout(.5))\n",
    "nn.add(Dense(units = 26, kernel_initializer = 'normal'))\n",
    "nn.add(PReLU())\n",
    "nn.add(BatchNormalization())\n",
    "nn.add(Dropout(.6))\n",
    "nn.add(Dense(1, kernel_initializer='normal'))\n",
    "nn.compile(loss='mae', optimizer=Adam(lr=4e-3, decay=1e-4))\n",
    "\n",
    "print(\"\\nFitting neural network model...\")\n",
    "nn.fit(np.array(x_train), np.array(y_train), batch_size = 32, epochs = 70, verbose=2)\n",
    "\n",
    "print(\"\\nPredicting with neural network model...\")\n",
    "#print(\"x_test.shape:\",x_test.shape)\n",
    "y_pred_ann = nn.predict(x_test)\n",
    "\n",
    "print( \"\\nPreparing results for write...\" )\n",
    "nn_pred = y_pred_ann.flatten()\n",
    "print( \"Type of nn_pred is \", type(nn_pred) )\n",
    "print( \"Shape of nn_pred is \", nn_pred.shape )\n",
    "\n",
    "print( \"\\nNeural Network predictions:\" )\n",
    "print( pd.DataFrame(nn_pred).head() )\n",
    "\n",
    "\n",
    "# Cleanup\n",
    "del train\n",
    "del prop\n",
    "del sample\n",
    "del x_train\n",
    "del x_test\n",
    "del df_train\n",
    "del df_test\n",
    "del y_pred_ann\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################\n",
    "################\n",
    "##    OLS     ##\n",
    "################\n",
    "################\n",
    "\n",
    "# This section is derived from the1owl's notebook:\n",
    "#    https://www.kaggle.com/the1owl/primer-for-the-zillow-pred-approach\n",
    "# which I (Andy Harless) updated and made into a script:\n",
    "#    https://www.kaggle.com/aharless/updated-script-version-of-the1owl-s-basic-ols\n",
    "\n",
    "np.random.seed(17)\n",
    "random.seed(17)\n",
    "\n",
    "print( \"\\n\\nProcessing data for OLS ...\")\n",
    "\n",
    "train = pd.read_csv(\"../Data/train_2016_v2.csv\", parse_dates=[\"transactiondate\"])\n",
    "properties = pd.read_csv(\"../Data/properties_2016.csv\")\n",
    "submission = pd.read_csv(\"../Data/sample_submission.csv\")\n",
    "print(len(train),len(properties),len(submission))\n",
    "\n",
    "def get_features(df):\n",
    "    df[\"transactiondate\"] = pd.to_datetime(df[\"transactiondate\"])\n",
    "    df[\"transactiondate_year\"] = df[\"transactiondate\"].dt.year\n",
    "    df[\"transactiondate_month\"] = df[\"transactiondate\"].dt.month\n",
    "    df['transactiondate'] = df['transactiondate'].dt.quarter\n",
    "    df = df.fillna(-1.0)\n",
    "    return df\n",
    "\n",
    "def MAE(y, ypred):\n",
    "    #logerror=log(Zestimate)−log(SalePrice)\n",
    "    return np.sum([abs(y[i]-ypred[i]) for i in range(len(y))]) / len(y)\n",
    "\n",
    "train = pd.merge(train, properties, how='left', on='parcelid')\n",
    "y = train['logerror'].values\n",
    "test = pd.merge(submission, properties, how='left', left_on='ParcelId', right_on='parcelid')\n",
    "properties = [] #memory\n",
    "\n",
    "exc = [train.columns[c] for c in range(len(train.columns)) if train.dtypes[c] == 'O'] + ['logerror','parcelid']\n",
    "col = [c for c in train.columns if c not in exc]\n",
    "\n",
    "train = get_features(train[col])\n",
    "test['transactiondate'] = '2016-01-01' #should use the most common training date\n",
    "test = get_features(test[col])\n",
    "\n",
    "\n",
    "print(\"\\nFitting OLS...\")\n",
    "reg = LinearRegression(n_jobs=-1)\n",
    "reg.fit(train, y); print('fit...')\n",
    "print(MAE(y, reg.predict(train)))\n",
    "train = [];  y = [] #memory\n",
    "\n",
    "test_dates = ['2016-10-01','2016-11-01','2016-12-01','2017-10-01','2017-11-01','2017-12-01']\n",
    "test_columns = ['201610','201611','201612','201710','201711','201712']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################\n",
    "########################\n",
    "##  Combine and Save  ##\n",
    "########################\n",
    "########################\n",
    "\n",
    "\n",
    "##### COMBINE PREDICTIONS\n",
    "\n",
    "print( \"\\nCombining XGBoost, LightGBM, NN, and baseline predicitons ...\" )\n",
    "lgb_weight = 1 - XGB_WEIGHT - BASELINE_WEIGHT - NN_WEIGHT - OLS_WEIGHT \n",
    "lgb_weight0 = lgb_weight / (1 - OLS_WEIGHT)\n",
    "xgb_weight0 = XGB_WEIGHT / (1 - OLS_WEIGHT)\n",
    "baseline_weight0 =  BASELINE_WEIGHT / (1 - OLS_WEIGHT)\n",
    "nn_weight0 = NN_WEIGHT / (1 - OLS_WEIGHT)\n",
    "pred0 = 0\n",
    "pred0 += xgb_weight0*xgb_pred\n",
    "pred0 += baseline_weight0*BASELINE_PRED\n",
    "pred0 += lgb_weight0*p_test\n",
    "pred0 += nn_weight0*nn_pred\n",
    "\n",
    "print( \"\\nCombined XGB/LGB/NN/baseline predictions:\" )\n",
    "print( pd.DataFrame(pred0).head() )\n",
    "\n",
    "print( \"\\nPredicting with OLS and combining with XGB/LGB/NN/baseline predicitons: ...\" )\n",
    "for i in range(len(test_dates)):\n",
    "    test['transactiondate'] = test_dates[i]\n",
    "    pred = FUDGE_FACTOR * ( OLS_WEIGHT*reg.predict(get_features(test)) + (1-OLS_WEIGHT)*pred0 )\n",
    "    submission[test_columns[i]] = [float(format(x, '.4f')) for x in pred]\n",
    "    print('predict...', i)\n",
    "\n",
    "print( \"\\nCombined XGB/LGB/NN/baseline/OLS predictions:\" )\n",
    "print( submission.head() )\n",
    "\n",
    "\n",
    "\n",
    "##### WRITE THE RESULTS\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "print( \"\\nWriting results to disk ...\" )\n",
    "submission.to_csv('sub{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False)\n",
    "\n",
    "print( \"\\nFinished ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
